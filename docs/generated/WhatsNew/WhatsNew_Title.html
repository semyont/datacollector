
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="What's New" /><meta name="abstract" content="" /><meta name="description" content="Data Collector version 2.4.0.0 includes the following new features and enhancements: Pipeline Sharing and Permissions Data Collector now provides pipeline-level permissions. Permissions determine the ..." /><meta name="DC.Relation" scheme="URI" content="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" /><meta name="DC.Relation" scheme="URI" content="../Installation/Install_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hz3_5fk_fy" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>What's New</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Installation/Install_title.html" title="Installation"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_hz3_5fk_fy">
 <h1 class="title topictitle1">What's New</h1>

 
 <div class="body conbody"><p class="shortdesc"></p>

  <p class="p"></p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_kzc_4sd_yy">
 <h2 class="title topictitle2">What's New in 2.4.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data
                  Collector</span>
            version 2.4.0.0 includes the following new features and enhancements:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Sharing and Permissions</dt>

                    <dd class="dd">Data Collector now provides pipeline-level permissions. Permissions
                        determine the access level that users and groups have on pipelines. To
                        create a multitenant environment, create groups of users and then share
                        pipelines with the groups to grant different levels of access.</dd>

                    <dd class="dd">With this change, only the pipeline owner and users with the Admin role can
                        view a pipeline by default. If upgrading from a previous version of Data
                        Collector, see the following post-upgrade task, <a class="xref" href="../Upgrade/PostUpgrade.html#concept_zbn_fpw_xy">Configure Pipeline Permissions</a>.</dd>

                    <dd class="dd">This feature includes the following components:<ul class="ul" id="concept_kzc_4sd_yy__ul_qhv_ds1_cz">
                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">Pipeline permissions</a> - Pipelines now have read, write,
                                and execute permissions. Pipeline permissions overlay existing Data
                                Collector roles to provide greater security. For information, see
                                    <a class="xref" href="../Configuration/RolesandPermissions.html#concept_k1r_prc_yy">Roles and Permissions</a>.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_jrg_1vy_wy">Pipeline sharing</a> - The pipeline owner and users with the
                                Admin role can configure pipeline permissions for users and
                                groups.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector pipeline access control property</a> - You
                                can enable and disable the use of pipeline permissions with the
                                pipeline.access.control.enabled configuration property. By default,
                                this property is enabled.</li>

                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_p11_msc_1z">Permissions transfer</a> - You can transfer all pipeline
                                permissions associated with a user or group to a different user or
                                group. Use pipeline transfer to easily migrate permissions after
                                registering with DPM or after a user or group becomes obsolete.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_f3t_zs1_cz">
                            <li class="li"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#task_a4y_v1g_xw" title="You can register a Data Collector with DPM from the Data Collector console.">Register Data Collectors with DPM</a> - If Data Collector
                                uses file-based authentication and if you register the Data
                                Collector from the Data Collector UI, you can now create DPM user
                                accounts and groups during the registration process.</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_c53_pzp_yy" title="When you write statistics to SDC RPC, Data Collector effectively adds an SDC RPC destination to the pipeline that you are configuring. The system pipeline is a pipeline with a Dev SDC RPC with Buffering origin that reads the statistics passed from the SDC RPC destination, and then aggregates and sends the statistics to DPM.">Aggregated statistics for DPM</a> - When working with DPM,
                                you can now configure a pipeline to write aggregated statistics to
                                SDC RPC. Write statistics to SDC RPC for development purposes only.
                                For a production environment, use a Kafka cluster or Amazon Kinesis
                                Streams to aggregate statistics.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_ad3_kt1_cz">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev SDC RPC with Buffering origin</a> - A new development
                                stage that receives records from an SDC RPC destination, temporarily
                                buffering the records to disk before passing the records to the next
                                stage in the pipeline. Use as the origin in an SDC RPC destination
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can configure a new
                                File Pool Size property to determine the maximum number of files
                                that the origin stores in memory for processing after loading and
                                sorting all files present on S3.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Other</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_hsj_tt1_cz">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release supports the
                                following new stage libraries:<ul class="ul" id="concept_kzc_4sd_yy__ul_xrv_5t1_cz">
                                    <li dir="ltr" class="li">Kudu versions 1.1 and 1.2</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">Cloudera CDH version 5.10 distribution of
                                            Hadoop</p>

                                    </li>

                                    <li dir="ltr" class="li">Cloudera version 5.10 distribution of Apache Kafka
                                        2.1</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="Install external libraries to make them available to Data Collector stages.">Install external libraries using the Data Collector user
                                    interface</a> - You can now use the Data Collector user
                                interface to install external libraries to make them available to
                                stages. For example, you can install JDBC drivers for stages that
                                use JDBC connections. Or, you can install external libraries to call
                                external Java code from the Groovy, Java, and Jython Evaluator
                                processors.</li>

                            <li class="li">Security enhancement - With this release, the ability to view log
                                information is restricted to users with the Admin role. For
                                information about upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_id5_lkk_yy">Enable Users to Access Data Collector Logs</a>.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Custom header enhancement</a> - You can now use HTML in the
                                ui.header.title configuration property to configure a custom header
                                for the <span class="ph">Data
                  Collector</span> console. This allows you to specify the look and feel for any
                                text that you use, and to include small images in the header. </li>

                            <li class="li"><a class="xref" href="../Processors/Groovy.html#task_asl_bpt_gv">Groovy enhancement</a> - You can configure the processor to
                                use the invokedynamic bytecode instruction.</li>

                            <li class="li">Pipeline renaming - You can now rename a pipeline by clicking
                                directly on the pipeline name when editing the pipeline, in addition
                                to editing the Title general pipeline property.</li>

                        </ul>

                    </dd>

                
            </dl>

        </div>

 </div>

</div>
<div class="topic concept nested1" id="concept_bml_dbt_wy">
 <h2 class="title topictitle2">What's New in 2.3.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.3.0.1 includes the following new features and enhancements:<ul class="ul" id="concept_bml_dbt_wy__ul_srs_hbt_wy">
                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC Client
                        origin enhancement</a> - The origin can now track and adapt to schema
                    changes when reading the dictionary from redo logs. When using the dictionary in
                    redo logs, the origin can also generate events for each DDL that it reads. </li>

                <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New
                        Data Collector property</a> - The http.enable.forwarded.requests property
                    in the Data Collector configuration file enables handling X-Forwarded-For,
                    X-Forwarded-Proto, X-Forwarded-Port request headers issued by a reverse proxy or
                    load balancer.</li>

                <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB
                        origin enhancement</a> ­ The origin now supports using any string field
                    as the offset field. </li>

            </ul>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_yym_xqt_5y">
 <h2 class="title topictitle2">What's New in 2.3.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.3.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">You can use a multithreaded origin to generate <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded pipelines</a> to perform parallel processing. <p dir="ltr" class="p">The new multithreaded framework includes the following
                            changes:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_slg_mrt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">HTTP
                                        Server origin</a> - Listens on an HTTP endpoint and
                                    processes the contents of all authorized HTTP POST requests. Use
                                    the HTTP Server origin to receive high volumes of HTTP POST
                                    requests using multiple threads.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Enhanced Dev Data Generator origin</a> - Can create
                                    multiple threads for testing multithreaded pipelines.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Enhanced runtime statistics</a> - Monitoring a pipeline
                                    displays aggregated runtime statistics for all threads in the
                                    pipeline. You can also view the number of runners, i.e. threads
                                    and pipeline instances, being used.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">CDC/CRUD Enhancements</dt>

                    <dd class="dd">With this release, certain Data Collector stages enable you to easily
                        process change data capture (CDC) or transactional data in a pipeline. The
                        sdc.operation.type record header attribute is now used by all CDC-enabled
                        origins and CRUD-enabled stages:<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_iws_mhd_ty">CDC-enabled origins</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_vb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB Oplog and Salesforce origins are now
                                    enabled for processing changed data by including the CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p">Though previously CDC-enabled, the Oracle CDC Client and JDBC
                                    Query Consumer for Microsoft SQL Server now include CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                                <p class="p">Previous operation type header attributes are still supported for
                                    backward-compatibility. </p>

                            </li>

                        </ul>
<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_lfb_phd_ty">CRUD-enabled stages</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_wb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The JDBC Tee processor and JDBC Producer can now
                                    process changed data based on CRUD operations in record headers.
                                    The stages also include a default operation and unsupported
                                    operation handling. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB and Elasticsearch destinations now look for
                                    the CRUD operation in the sdc.operation.type record header
                                    attribute. The Elasticsearch destination includes a default
                                    operation and unsupported operation handling.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Multitable Copy</dt>

                    <dd class="dd">You can use the new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y" title="The JDBC Multitable Consumer origin reads database data from multiple tables through a JDBC connection. The origin returns data as a map with column names and field values.">JDBC
                            Multitable Consumer origin</a> when you need to copy multiple tables
                        to a destination system or for database replication. The JDBC Multitable
                        Consumer origin reads database data from multiple tables through a JDBC
                        connection. The origin generates SQL queries based on the table
                        configurations that you define.</dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_up5_ntt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Configuration/Authentication.html#concept_wgy_rxt_5x" title="If your organization does not use LDAP, configure Data Collector to use the default file-based authentication.">Groups for file-based authentication</a> - If you use
                                file-based authentication, you can now create groups of users when
                                multiple users use Data Collector. You configure groups in the
                                associated realm.properties file located in the Data Collector
                                configuration directory, $SDC_CONF. <p class="p">If you use file-based
                                    authentication, you can also now view all user accounts granted
                                    access to the Data Collector, including the roles and groups
                                    assigned to each user.</p>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/Authentication.html#concept_dns_dvg_h5" title="Data Collector can authenticate user accounts based on LDAP or files. Best practice is to use LDAP if your organization has it. By default, Data Collector uses file-based authentication.">LDAP authentication enhancements</a> - You can now
                                    configure Data Collector to use StartTLS to make secure
                                    connections to an LDAP server. You can also configure the
                                    userFilter property to define the LDAP user attribute used to
                                    log in to Data Collector. For example, a username, uid, or email
                                    address.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#concept_dmr_df5_5y" title="You can configure each registered Data Collector to use an authenticated HTTP or HTTPS proxy server for outbound requests made to DPM. Define the proxy properties in the SDC_JAVA_OPTS environment variable in the Data Collector environment configuration file.">Proxy
                                        configuration for outbound requests</a> - You can now
                                    configure Data Collector to use an authenticated HTTP proxy for
                                    outbound requests to Dataflow Performance Manager (DPM).</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector logging</a> - Data Collector now
                                    enables logging for the Java garbage collector by default. Logs
                                    are written to $SDC_LOG/gc.log. You can disable the logging if
                                    needed. </p>

                            </li>

                            <li dir="ltr" class="li">Heap dump for out of memory errors - Data Collector now
                                produces a heap dump file by default if it encounters an out of
                                memory error. You can configure the location of the heap dump file
                                or you can disable this default behavior. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Administration/Administration_title.html#task_lkv_g2f_wy" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Modifying the log level</a> - You can now use the Data
                                Collector UI to modify the log level to display messages at another
                                severity level.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_lzx_xtt_5y">
                            <li dir="ltr" class="li">Pipeline renaming - You can now rename pipelines by
                                editing the Title general pipeline property.</li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z">Field attributes</a> - Data Collector now supports
                                    field-level attributes. Use the Expression Evaluator to add
                                    field attributes.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_ljw_15t_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">New HTTP Server origin</a> - A multithreaded origin that
                                listens on an HTTP endpoint and processes the contents of all
                                authorized HTTP POST requests. Use the HTTP Server origin to read
                                high volumes of HTTP POST requests using multiple threads. </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPtoKafka.html#concept_izh_mqd_dy">New
                                        HTTP to Kafka origin</a> - Listens on a HTTP endpoint and
                                    writes the contents of all authorized HTTP POST requests
                                    directly to Kafka. Use to read high volumes of HTTP POST
                                    requests and write them to Kafka. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MapRDBJSON.html#concept_ywh_k15_3y" title="The MapR DB JSON origin reads JSON documents from MapR DB JSON tables. The origin converts each document into a record.">New
                                        MapR DB JSON origin</a> - Reads JSON documents from MapR
                                    DB JSON tables.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MongoDBOplog.html#concept_mjn_yqw_4y">New
                                        MongoDB Oplog origin</a> - Reads entries from a MongoDB
                                    Oplog. Use to process change information for data or database
                                    operations. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/Directory.html#concept_xd5_5z4_4y">Directory origin enhancement</a> - You can use regular
                                    expressions in addition to glob patterns to define the file name
                                    pattern to process files. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPClient.html#concept_c13_zz1_5y" title="You can configure the HTTP Client origin to use the OAuth 2 protocol to connect to an HTTP service that uses basic, digest, or universal authentication, OAuth 2 client credentials, OAuth 2 username and password, or OAuth 2 JSON Web Tokens (JWT).">HTTP Client origin enhancement</a> - You can now
                                    configure the origin to use the OAuth 2 protocol to connect to
                                    an HTTP service.</p>

                            </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs" title="JDBC Query Consumer uses an offset column and initial offset value to determine where to start reading data within a table. Include both the offset column and the offset value in the WHERE clause of the SQL query. JDBC Query Consumer supports recovery after a deliberate or unexpected stop when it performs incremental queries. Recovery is not supported for full queries.When you define the SQL query for incremental mode, JDBC Query Consumer requires a WHERE and ORDER BY clause in the query. You can define any type of SQL query for full mode.">JDBC
                                    Query Consumer origin enhancements</a> - The JDBC Consumer
                                origin has been renamed to the JDBC Query Consumer origin. The
                                origin functions the same as in previous releases. It reads database
                                data using a user-defined SQL query through a JDBC connection. You
                                can also now configure the origin to enable auto-commit mode for the
                                JDBC connection and to disable validation of the SQL query.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_bk4_2rs_ns">MongoDB
                                    origin enhancements</a> - You can now use a nested field as
                                the offset field. The origin supports reading the MongoDB BSON
                                timestamp for MongoDB versions 2.6 and later. And you can configure
                                the origin to connect to a single MongoDB server or node. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_z35_cwt_5y">
                            <li dir="ltr" class="li">Field Type Converter processor enhancement - You can now
                                configure the processor to convert timestamp data in a Long field to
                                a String. Previously, you had to use one Field Type Converter
                                processor to convert the Long field to a Datetime, and then use
                                another processor to convert the Datetime field to a String.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/HTTPClient.html#concept_ghx_ypr_fw">HTTP Client processor enhancements</a>  - You can now
                                    configure the processor to use the OAuth 2 protocol to connect
                                    to an HTTP service. You can also configure a rate limit for the
                                    processor, which defines the maximum number of requests to make
                                    per second.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw" title="The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data.">JDBC Lookup processor enhancements</a> - You can now
                                    configure the processor to enable auto-commit mode for the JDBC
                                    connection. You can also configure the processor to use a
                                    default value if the database does not return a lookup value for
                                    a column.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor enhancement</a> - You can
                                    now configure the processor to use a default value if Salesforce
                                    does not return a lookup value for a field.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5" title="Configure an XML Parser to parse XML data in a string field.">XML
                                        Parser enhancement</a> - A new Multiple Values Behavior
                                    property allows you to specify the behavior when you define a
                                    delimiter element and the document includes more than one value:
                                    Return the first value as a record, return one record with a
                                    list field for each value, or return all values as records.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_uys_hwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#concept_i4h_2kj_dy" title="The MapR DB JSON destination writes data as JSON documents to MapR DB JSON tables. The destination converts each record into a JSON document and writes the document to the JSON table that you specify.">New
                                    MapR DB JSON destination</a> - Writes data as JSON documents
                                to MapR DB JSON tables.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> enhancement - You
                                    can now use the destination in cluster batch pipelines. You can
                                    also process binary and protobuf data, use record header
                                    attributes to write records to files and roll files, and
                                    configure a file suffix and the maximum number of records that
                                    can be written to a file. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Elasticsearch.html#concept_u5t_vpv_4r" title="The Elasticsearch destination writes data to an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters) and Shield-enabled clusters. The destination uses the Elasticsearch HTTP API to write each record to Elasticsearch as a document.">Elasticsearch destination enhancement</a> - The
                                    destination now uses the Elasticsearch HTTP API. With this API,
                                    the Elasticsearch version 5 stage library is compatible with all
                                    versions of Elasticsearch. Earlier stage library versions have
                                    been removed. Elasticsearch is no longer supported on Java 7.
                                    You’ll need to verify that Java 8 is installed on the Data
                                    Collector machine and remove this stage from the blacklist
                                    property in $SDC_CONF/sdc.properties before you can use it. </p>

                                <p class="p">You can also now configure the destination to perform any of the
                                    following CRUD operations: create, update, delete, or index.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HiveMetastore.html#concept_x4p_fyc_rx">Hive Metastore destination enhancement</a> - New table
                                    events now include information about columns and partitions in
                                    the table.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_uv2_vfb_vy">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_s5n_ggc_vy">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_vvr_ngc_vy">MapR FS</a> destination enhancement - The destinations
                                    now support recovery after an unexpected stop of the pipeline by
                                    renaming temporary files when the pipeline restarts.</p>

                            </li>

                            <li dir="ltr" class="li">Redis destination enhancement - You can now configure a
                                timeout for each key that the destination writes to Redis.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_zqk_4wt_5y">
                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive
                                        Query executor enhancements</a>: <ul class="ul" id="concept_yym_xqt_5y__ul_gf2_qwt_5y">
                                        <li class="li">The executor can now execute multiple queries for each
                                            event that it receives.</li>

                                        <li class="li">It can also generate event records each time it
                                            processes a query.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                        <ul class="ul" id="concept_yym_xqt_5y__ul_brk_4wt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC
                                        Query executor enhancement</a> - You can now configure
                                    the executor to enable auto-commit mode for the JDBC
                                    connection.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hpw_zwt_5y">
                            <li class="li"><a class="xref" href="../Pipeline_Design/WholeFile.html#concept_prp_jzd_py">Whole File enhancement</a> - You can now specify a transfer
                                rate to help control the resources used to process whole files. You
                                can specify the rate limit in all origins that process whole
                                files.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hq3_2xt_5y">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline functions</a> - You can use the following new
                                pipeline functions to return pipeline information:<ul class="ul" id="concept_yym_xqt_5y__ul_mls_bp1_cz">
                                    <li class="li">pipeline:id() - Returns the pipeline ID, a UUID that is
                                        automatically generated and used by Data Collector to
                                        identify the pipeline. <div class="note note"><span class="notetitle">Note:</span> The existing pipeline:name()
                                            function now returns the pipeline ID instead of the
                                            pipeline name since pipeline ID is the correct way to
                                            identify a pipeline.</div>
</li>

                                    <li class="li">
                                        <p class="p">pipeline:title() - Returns the pipeline title or
                                            name.</p>

                                    </li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_p1z_ggv_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record.">New record functions</a> - You can use the following new
                                    record functions to work with field attributes:<ul class="ul" id="concept_yym_xqt_5y__ul_k4h_kxt_5y">
                                        <li class="li">record:fieldAttribute (&lt;field path&gt;, &lt;attribute
                                            name&gt;) - Returns the value of the specified field
                                            attribute. </li>

                                        <li class="li">record:fieldAttributeOrDefault (&lt;field path&gt;,
                                            &lt;attribute name&gt;, &lt;default value&gt;) - Returns the
                                            value of the specified field attribute. Returns the
                                            default value if the attribute does not exist or
                                            contains no value.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - You can use the following new
                                    string functions to transform string data: <ul class="ul" id="concept_yym_xqt_5y__ul_knh_mxt_5y">
                                        <li class="li">str:urlEncode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            URL encoded string from a decoded string using the
                                            specified encoding format. </li>

                                        <li class="li">str:urlDecode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            decoded string from a URL encoded string using the
                                            specified encoding format.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New time functions</a> - You can use the following new
                                    time functions to transform datetime data: <ul class="ul" id="concept_yym_xqt_5y__ul_yk5_pxt_5y">
                                        <li class="li">time:dateTimeToMilliseconds (&lt;Date object&gt;) -
                                            Converts a Date object to an epoch or UNIX time in
                                            milliseconds. </li>

                                        <li class="li">time:extractDateFromString(&lt;string&gt;, &lt;format
                                            string&gt;) - Extracts a Date object from a String, based
                                            on the specified date format. </li>

                                        <li class="li">time:extractStringFromDateTZ (&lt;Date object&gt;,
                                            &lt;timezone&gt;, &lt;format string&gt;) - Extracts a string
                                            value from a Date object based on the specified date
                                            format and time zone.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New and enhanced miscellaneous functions</a> - You can
                                    use the following new and enhanced miscellaneous functions: <ul class="ul" id="concept_yym_xqt_5y__ul_m2x_nxt_5y">
                                        <li class="li">offset:column(&lt;position&gt;) - Returns the value of the
                                            positioned offset column for the current table.
                                            Available only in the additional offset column
                                            conditions of the JDBC Multitable Consumer origin. </li>

                                        <li class="li">every function - You can now use the function with the
                                            hh() datetime variable in directory templates. This
                                            allows you to create directories based on the specified
                                            interval for hours.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_wbf_dgk_fy">
 <h2 class="title topictitle2">What's New in 2.2.1.0</h2>

 <div class="body conbody">
        <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.2.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_y1c_1pm_3y">
                            <li class="li">New <a class="xref" href="../Processors/FieldZip.html#concept_o3b_t1k_yx">Field Zip processor</a> - Merges two List fields or two
                                List-Map fields in the same record.</li>

                            <li class="li">New <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a> - Performs lookups in a
                                Salesforce object and passes the lookup values to fields. Use the
                                Salesforce Lookup to enrich records with additional data.</li>

                            <li class="li"><a class="xref" href="../Processors/ValueReplacer.html#concept_ppg_ztk_3y">Value Replacer</a> enhancement - You can now replace field
                                values with nulls using a condition.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_vlq_dpm_3y">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_wsz_qj4_zx">Whole file support in the Azure Data Lake Store
                                    destination</a> - You can now use the whole file data format
                                to stream whole files to Azure Data Lake Store. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_oyv_zfk_fy">
 <h2 class="title topictitle2">What's New in 2.2.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span> version
            2.2.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Event Framework</dt>

                    <dd class="dd">The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>

                    <dd class="dd">For details, see the <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Event Framework chapter</a>. </dd>

                    <dd class="dd">The event framework includes the following new features and enhancements:<ul class="ul" id="concept_oyv_zfk_fy__ul_ojf_xgk_fy">
                            <li class="li"><a class="xref" href="../Executors/Executors-overview.html#concept_stt_2lk_fx">New executor stages</a>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul class="ul" id="concept_oyv_zfk_fy__ul_bn4_ygk_fy">
                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File Metadata executor</a> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>

                                    <li class="li"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive Query executor</a> - Runs a Hive or Impala
                                        query. </li>

                                    <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor</a> - Runs a SQL query. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce executor</a> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>

                                </ul>
</li>

                            <li class="li">Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_sxd_bhk_fy">
                                    <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory</a> and <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">File Tail</a> origins - Generate events when they
                                        start and complete reading a file.</li>

                                    <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">Amazon S3 destination</a> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">MapR FS</a> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors - Can run scripts
                                        that generate events. </li>

                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">HDFS File Metadata executor</a> - Generates events
                                        when it changes file metadata. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">MapReduce executor</a> - Generates events when it
                                        starts a MapReduce job.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev stages</a>. You can use the following stages to develop
                                and test event handling: <ul class="ul" id="concept_oyv_zfk_fy__ul_ysc_2hk_fy">
                                    <li class="li">Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>

                                    <li class="li">To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_q3x_pvm_3y">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Installation requirements</a>:<ul class="ul" id="concept_oyv_zfk_fy__ul_zh5_qvm_3y">
                                    <li class="li">Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>

                                    <li class="li">File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>

                                </ul>
</li>

                        </ul>

                        <ul class="ul" id="concept_oyv_zfk_fy__ul_jtf_ghk_fy">
                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Core installation</a> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p class="p">Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5w_mhk_fy">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a>. Data Collector now supports the
                                following stage libraries: <ul class="ul" id="concept_oyv_zfk_fy__ul_oxv_nhk_fy">
                                    <li class="li">Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>

                                    <li class="li">Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>

                                    <li class="li">Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>

                                    <li class="li">Elasticsearch version 5.0.x. </li>

                                    <li class="li">Google Cloud Bigtable. </li>

                                    <li class="li">Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>

                                    <li class="li">MySQL Binary Log. </li>

                                    <li class="li">Salesforce. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP, configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication</a> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>

                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector</a> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>

                            <li class="li">Environment variables for <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_vrx_4fg_qr" title="You can define the Data Collector Java heap size. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. To enable remote debugging, define debugging options in the SDC_JAVA_OPTS environment variable in the Data Collector environment configuration file.You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.When you use Data Collector with Java 7, Data Collector is configured to use TLS versions 1.1 and 1.2. To connect to a system that uses an earlier version of TLS, modify the Dhttps.protocols option in the SDC_JAVA7_OPTS environment variable in the Data Collector environment configuration file.When you use Data Collector with Java 7, you can define the Java Permanent Generation size, also known as the PermGen size.">Java configuration options</a>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul class="ul" id="concept_oyv_zfk_fy__ul_wym_thk_fy">
                                    <li class="li">SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>

                                    <li class="li">SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">New time zone property</a> - You can configure the Data
                                Collector console to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_kq1_whk_fy">
                            <li class="li">New <a class="xref" href="../Origins/MySQLBinaryLog.html#concept_kqg_1yh_xx" title="You can configure the origin to start reading the binary log file from the beginning of the file or from an initial offset in the file.The binary log file captures all changes made to the MySQL database. If you want the MySQL Binary Log origin to capture changes from a subset of tables, you can configure the origin to include changes from specific tables or to ignore changes from specific tables.">MySQL Binary Log origin</a> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>

                            <li class="li">New <a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx" title="The Salesforce origin reads data from Salesforce.">Salesforce origin </a>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>

                            <li class="li"><a class="xref" href="../Origins/Directory.html#concept_qpt_rg3_cy">Directory origin</a> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>

                            <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer</a> and <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Query
                                Consumer and Oracle CDC Client origins use to connect to the
                                database. Previously, the origins used the default transaction
                                isolation level configured for the database.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_wfg_13k_fy">
                            <li class="li">New <a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. Use the Spark Evaluator processor in standalone pipelines only.">Spark
                                    Evaluator processor</a> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#concept_vpx_zc1_xx">Field Flattener processor</a> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_sym_c4g_xx" title="You can use the Field Type Converter to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you'd like to decrease the scale to 4 so that the value is 12345.6789.">Field Type Converter processor</a> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>

                            <li class="li"><a class="xref" href="../Processors/ListPivoter.html#concept_ekg_313_qw" title="Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field.">Field
                                    Pivoter processor</a> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>

                            <li class="li">Scripting processor enhancements - The <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLFlattener.html#task_pmb_l55_sv" title="Configure an XML Flattener to flatten XML data embedded in a string field.">XML Flattener processor</a> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5" title="Configure an XML Parser to parse XML data in a string field.">XML
                                    Parser processor</a> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5j_l3k_fy">
                            <li class="li">New <a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> - Writes data to
                                Microsoft Azure Data Lake Store. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Bigtable.html#concept_pl5_tmq_tx" title="The Google Bigtable destination writes data to Google Cloud Bigtable.">Google Bigtable destination</a> - Writes data to Google
                                Cloud Bigtable. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination</a> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>

                            <li class="li">File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq" title="You can use Kerberos authentication to connect to HDFS. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to HDFS. You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS.">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                    FS</a>, <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR
                                    FS</a>, and the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt" title="The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination.">Amazon
                                    S3</a> destinations. </li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer</a> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#concept_dvg_vvj_wx">Kudu destination</a> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_r3d_dkk_fy">
                            <li class="li"><a class="xref" href="../Pipeline_Design/XMLDFormat.html#concept_lty_42b_dy">XML processing</a> enhancement - You can now generate
                                records from XML documents using <a class="xref" href="../Pipeline_Design/XMLDFormat.html#concept_zw2_mfk_dy">simplified XPath expressions</a> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>

                            <li class="li">Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p class="p">Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/WholeFile.html#concept_ojv_sr4_vx">Checksum generation for whole files</a> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Maintenance</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_h24_3kk_fy">
                            <li class="li">Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>

                            <li class="li">Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Rules and Alerts</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_sfk_lkk_fy">
                            <li class="li"><a class="xref" href="../Alerts/RulesAlerts_title.html#concept_ky2_g4f_qv" title="The gauge metric type provides alerts based on the number of input, output, or error records for the last processed batch. It also provides alerts on the age of the current batch, the amount of time a stage takes to process a batch, or the time that Data Collector last received a record from the origin.">Metric
                                    rules and alerts</a> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language Functions</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_bwr_nkk_fy">
                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_kxj_nyl_5x">file functions </a>- You can use the following new file
                                functions to work with file paths:<ul class="ul" id="concept_oyv_zfk_fy__ul_xdq_4kk_fy">
                                    <li class="li">file:fileExtension(&lt;filepath&gt;) - Returns the file
                                        extension from a path. </li>

                                    <li class="li">file:fileName(&lt;filepath&gt;) - Returns a file name from a
                                        path. </li>

                                    <li class="li">file:parentPath(&lt;filepath&gt;) - Returns the parent path of
                                        the specified file or directory. </li>

                                    <li class="li">file:pathElement(&lt;filepath&gt;, &lt;integer&gt;) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>

                                    <li class="li">file:removeExtension(&lt;filepath&gt;) - Removes the file
                                        extension from a path. </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">pipeline functions</a> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_lc5_rkk_fy">
                                    <li class="li">pipeline:name() - Returns the pipeline name. </li>

                                    <li class="li">pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> - You can use the following new time
                                functions to transform datetime data:<ul class="ul" id="concept_oyv_zfk_fy__ul_znz_skk_fy">
                                    <li class="li"> time:extractLongFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:extractStringFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:millisecondsToDateTime(&lt;long&gt;) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Installation/Install_title.html" title="Installation"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>