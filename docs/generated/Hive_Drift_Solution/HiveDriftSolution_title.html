
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="description" content="The Drift Synchronization Solution for Hive detects drift in incoming data and updates corresponding Hive tables. Previously known as the Hive Drift Solution, the Drift Synchronization Solution for ..." /><meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Drift Synchronization Solution (a.k.a. Hive Drift Solution)" /><meta name="DC.Relation" scheme="URI" content="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" /><meta name="DC.Relation" scheme="URI" content="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_fjj_zcf_2w" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Drift Synchronization Solution (a.k.a. Hive Drift Solution)</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" title="Event Framework"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Event Framework</span></a></span>  
<span class="navnext"><a class="link" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" title="Multithreaded Pipelines"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Multithreaded Pipelines</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_fjj_zcf_2w">
 <h1 class="title topictitle1">Drift Synchronization Solution (a.k.a. Hive Drift Solution)</h1>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_phk_bdf_2w">
    <h2 class="title topictitle2">Drift Synchronization Solution for Hive</h2>

    <div class="body conbody">
        <p class="p">The
            Drift Synchronization Solution for Hive detects drift in incoming data and updates
            corresponding Hive tables. Previously known as the Hive Drift Solution, the Drift
            Synchronization Solution for Hive enables creating and updating Hive tables based on
            record requirements and writing data to HDFS or MapR FS based on record header
            attributes. You can use the full functionality of the solution or individual pieces, as
            needed. </p>

        <p class="p">The Drift Synchronization Solution for Hive supports only Avro data at this time. The
            solution is compatible with Impala.</p>

        <p class="p">The solution incorporates the Hive Metadata processor, Hive Metastore destination, and
            the Hadoop FS or MapR FS destination as follows:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Drift detection</dt>

                    <dd class="dd">When processing records, the Hive Metadata processor detects columnar drift
                        and the need for new tables and partitions. It generates metadata records
                        that describe the necessary changes and passes it to the Hive Metastore
                        destination.</dd>

                    <dd class="dd">When the Hive Metastore destination receives a metadata record, it compares
                        the proposed changes with the latest Hive metadata, and creates and updates
                        Hive tables as needed.</dd>

                    <dd class="dd"><span class="ph">The destination can
                        create tables and partitions. It can add columns to tables and ignore
                        existing columns. It does not drop existing columns from tables.</span></dd>

                
                
                    <dt class="dt dlterm">Record-based writes</dt>

                    <dd class="dd">The Hive Metadata processor also adds information to the header of each
                        record and passes the records to the Hadoop FS destination or the MapR FS
                        destination. The destinations can perform record-based writes to their
                        destination systems based on the following details: <ul class="ul" id="concept_phk_bdf_2w__ul_sn1_bjg_2w">
                            <li class="li">Target directory - Based on user-defined expressions, the Hive
                                Metadata processor assembles the path where each record should be
                                stored. It writes the generated path to a
                                    <dfn class="term">targetDirectory</dfn> attribute in each record
                                    header.<p class="p">To write the record to the generated path, configure
                                    the destination to use the targetDirectory header attribute.
                                </p>
</li>

                            <li class="li">Avro schema - The processor writes the Avro schema to the
                                    <dfn class="term">avroSchema</dfn> attribute in each record header. It
                                generates new Avro schemas when necessary based on the record
                                    structure.<p class="p">To use the generated Avro schema, configure the
                                    destination to use the avroSchema header attribute.</p>
</li>

                            <li class="li">Roll files - When a change in Avro schema occurs, the processor
                                generates a roll indicator - the <dfn class="term">roll</dfn> header attribute.
                                    <p class="p">To roll files based on schema changes, configure the
                                    destination use the roll header attribute.</p>
</li>

                        </ul>
</dd>

                
            </dl>

        </div>

        <p class="p">For example, say you use this solution to write sales data to MapR FS. A partial upgrade
            of the sales system adds several new fields to a subset of the incoming data. </p>

        <p class="p">With the Drift Synchronization Solution for Hive, the Hive Metadata processor notes the
            new fields in a metadata record and passes it to the Hive Metastore destination. The
            Hive Metastore destination adds the new columns to the Hive target table. The MapR FS
            destination then writes the data to the updated table. When writing data without the new
            fields to the updated table, the destination inserts null values for the missing fields. </p>

    </div>

<div class="topic concept nested2" id="concept_u2t_fgy_1x">
 <h3 class="title topictitle3">Impala Support</h3>

 <div class="body conbody">
  <p class="p">Data written by the Drift Synchronization Solution is
            compatible with Impala.</p>

        <p class="p">Impala requires using the Invalidate Metadata command to refresh the Impala metadata
            cache each time changes occur in the Hive Metastore. To automatically refresh the Impala
            metadata cache, add an event stream to your pipeline. For details, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_szz_xwm_lx">Case Study: Impala Metadata Updates for DDS for Hive</a>.</p>

        <p class="p">You can alternatively run the Impala Invalidate Metadata command manually. To be notified
            of when to run the command, add a data rule alert on the link to the Hive Metastore
            destination to let you know when metadata records are passed to the Hive Metastore. </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_s3v_21p_hx">
 <h3 class="title topictitle3">Flatten Records</h3>

 <div class="body conbody">
  <p class="p">At this time, the Drift Synchronization Solution does not process records with nested fields.
            If necessary, you can use the Field Flattener processor to flatten records with nested
            fields before passing them to the Hive Metadata processor. </p>

 </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br />
<div class="related_link"><a class="navheader_parent_path" href="../Processors/FieldFlattener.html#concept_njn_3kk_fx" title="Field Flattener">Field Flattener</a></div>
</div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_zzs_fkg_2w">
    <h2 class="title topictitle2">Basic Implementation</h2>

    <div class="body conbody">
        <p class="p">You can use the Hive Metadata processor,
            Hive Metastore destination for metadata processing, and Hadoop FS or MapR FS destination
            for data processing in any pipeline where the logic is appropriate.</p>

        <p class="p">A basic implementation of the Drift Synchronization Solution for Hive includes the origin
            of your choice and the Hive Metadata processor connected to the Hive Metastore
            destination and either the Hadoop FS or MapR FS destinations, as follows:</p>

        <p class="p"><img class="image" id="concept_zzs_fkg_2w__image_ys3_ztg_2w" src="../Graphics/HiveMeta-Pipeline.png" height="173" width="407" /></p>

        <p class="p"><img class="image" id="concept_zzs_fkg_2w__image_nrz_fdk_tw" src="../Graphics/HiveMeta-MapR-Pipeline.png" height="173" width="407" /></p>

        <p class="p">The Hive Metadata processor passes records through the first output stream - the data
            stream. Connect the data stream to the Hadoop FS or MapR FS destination to write data to
            the destination system using record header attributes. </p>

        <p class="p">The Hive Metadata processor passes the metadata record through the second output stream -
            the metadata output stream. Connect the Hive Metastore destination to the metadata
            output stream to enable the destination to create and update Hive tables. The metadata
            output stream contains no record data. </p>

        <p class="p">If your data contains nested fields, you would add a Field Flattener to flatten records
            as follows: </p>

        <p class="p"><img class="image" id="concept_zzs_fkg_2w__image_ebf_zkp_hx" src="../Graphics/HiveDrift-Flatten.png" height="161" width="469" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_y5w_dj3_fw">
 <h2 class="title topictitle2">Implementation Steps</h2>

 <div class="body conbody">
  <div class="p">To implement the Drift
            Synchronization Solution for Hive, perform the following steps:<ol class="ol" id="concept_y5w_dj3_fw__ol_zr1_3j3_fw">
                <li class="li"> Configure the origin and any additional processors that you want to use. <ul class="ul" id="concept_y5w_dj3_fw__ul_qjn_kp4_hx">
                        <li class="li">If using the JDBC Query Consumer as the origin, enable the creation of
                            JDBC header attributes. For more information, see <a class="xref" href="../Origins/JDBCConsumer.html#concept_tvf_tgp_fx">Header Attributes with the Drift Synchronization Solution</a>. </li>

                        <li class="li">If data includes records with nested fields, add a Field Flattener to
                            flatten records before passing them to the Hive Metadata processor.</li>

                    </ul>
</li>

                <li class="li">To capture columnar drift and to enable record-based writes, configure the Hive
                    Metadata processor:<ul class="ul" id="concept_y5w_dj3_fw__ul_qsk_5x3_fw">
                        <li class="li">Configure the Hive connection information.</li>

                        <li class="li">Configure the database, table, and partition expressions.<p class="p">You can
                                enter a single name or use an expression that evaluates to the names
                                to use. If necessary, you can use an Expression Evaluator earlier in
                                the pipeline to write the information to a record field or record
                                header attribute.</p>
</li>

                        <li class="li">Configure the decimal field precision and scale expressions.<p class="p">You can
                                use constants or expressions that evaluate to the same precision and
                                scale for all decimal fields. Or, you can create more complex
                                expressions that evaluate to different values for different fields.
                                </p>
<p class="p">When processing data from the JDBC Query Consumer or the JDBC
                                Multitable Consumer with JDBC header attributes, use the default
                                expressions. </p>
</li>

                        <li class="li">Optionally configure advanced options, such as the maximum cache size,
                            time basis, and data time zone.</li>

                    </ul>
<p class="p">For more information about the Hive Metadata processor, see <a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination, and the Hadoop FS or MapR FS destinations as part of the Drift Synchronization Solution for Hive.">Hive Metadata</a>.</p>
</li>

                <li class="li">To process metadata records generated by the processor and alter Hive tables as
                    needed, connect the metadata output of the Hive Metadata processor to the Hive
                    Metastore destination.<div class="p">
                        <div class="note note"><span class="notetitle">Note:</span> While you might filter or route some records away from the Hive
                            Metastore destination, the destination must receive metadata records to
                            update Hive tables.</div>

                    </div>
</li>

                <li class="li">Configure the Hive Metastore destination:<ul class="ul" id="concept_y5w_dj3_fw__ul_k12_ty3_fw">
                        <li class="li">Configure the Hive connection information.</li>

                        <li class="li">Optionally configure cache information and how tables are updated.</li>

                    </ul>
<p class="p">For more information about the Hive Metastore destination, see <a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore</a>.</p>
</li>

                <li class="li">Connect the data output of the Hive Metadata processor to the Hadoop FS or MapR
                    FS destination to write records to the destination system using record header
                    attributes.</li>

                <li class="li">Configure the Hadoop FS or MapR FS destination:<ul class="ul" id="concept_y5w_dj3_fw__ul_n4p_dz3_fw">
                        <li class="li">To write records using the targetDirectory header attribute, on the
                                <span class="keyword wintitle">Output Files</span> tab, select <span class="ph uicontrol">Directory
                                in Header</span>.</li>

                        <li class="li">To roll records based on a roll header attribute, on the
                                <span class="keyword wintitle">Output Files</span> tab, select <span class="ph uicontrol">Use Roll
                                Attribute</span>, and for <span class="ph uicontrol">Roll Attribute
                                Name</span>, enter “roll”.</li>

                        <li class="li">To write records using the avroSchema header attribute, on the
                                <span class="keyword wintitle">Data Format</span> tab, select the
                                <span class="ph uicontrol">Avro</span> data format, and then for the
                                <span class="ph uicontrol">Avro Schema Location</span> property, select
                                <span class="ph uicontrol">In Record Header</span>.</li>

                    </ul>
<p class="p">For more information about using record header attributes, see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_lmn_gdc_1w">Record Header Attributes for Record-Based Writes</a>.</p>
</li>

            </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_a1w_kkn_fw">
 <h2 class="title topictitle2">Case Study</h2>

 <div class="body conbody">
  <p class="p">Let's say you have a <span class="ph">Data
                  Collector</span>
            pipeline that writes log data to Kafka. The File Tail origin in the pipeline processes
            data from several different web services, tagging each record with a "tag" header
            attribute that identifies the service that generated the data. </p>

        <p class="p">Now you want a new pipeline to pass the data to HDFS where it can be stored and reviewed,
            and you'd like the data written to tables based on the web service that generated the
            data. Note that you could also write the data to MapR FS -- the steps are almost
            identical to this case study, you'd just use a different destination.</p>

        <p class="p">To do this, add and configure a Kafka Consumer to read the data into the pipeline, then
            connect it to a Hive Metadata processor. The processor assesses the record structure and
            generates a metadata record that describes any required Hive metadata changes. Using the
            tag header attribute and other user-defined expressions, a Hive Metadata processor can
            determine the database, table, and partition to use for the target directory and write
            that information along with the Avro schema to the record header, including file roll
            indicator when necessary.</p>

        <p class="p">You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Hive tables as needed. </p>

        <p class="p">You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record where it wants to go using the target directory and Avro schema in the record
            header, and rolling files when needed. </p>

        <p class="p">Now let's take a closer look... </p>

 </div>

<div class="topic concept nested2" id="concept_fzk_mmn_fw">
 <h3 class="title topictitle3">The Hive Metadata Processor</h3>

 <div class="body conbody">
  <div class="p">You set up the Kafka Consumer and connect it to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to your basic connection
            details: <ol class="ol" id="concept_fzk_mmn_fw__ol_fzm_bmv_fw">
                <li class="li">Which database should the records be written to? <p class="p">Hadoop FS will do the
                        writing, but the processor needs to know where the records should
                        go.</p>
<p class="p">Let's write to the Hive default database. To do that, you can
                        leave the database property empty.</p>
</li>

                <li class="li">What tables should the records be written to?<div class="p">The pipeline supplying the data
                        to Kafka uses the "tag" header attribute to indicate the originating web
                        service. To use the tag attribute to write to tables, you use the following
                        expression for the table name:
                        <pre class="pre codeblock">${record:attribute('tag')}</pre>
</div>
</li>

                <li class="li">What partitions, if any, do you want to use? <div class="p">Let's create daily partitions
                        using datetime variables for the partition value expression as
                        follows:<pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}</pre>
</div>
</li>

                <li class="li">How do you want to configure the precision and scale for decimal fields?
                        <p class="p">Though the data from the web services contains no decimal data that you
                        are aware of, to prevent new decimal data from generating error records,
                        configure the decimal field expressions. </p>
<p class="p">The default expressions are
                        for data generated by the JDBC Query Consumer or the JDBC Multitable
                        Consumer. You can replace them with other expressions or with constants.
                    </p>
</li>

            </ol>
</div>

        <p class="p"> At this point, your pipeline would look like this: </p>

        <p class="p"><img class="image" id="concept_fzk_mmn_fw__image_g5b_34n_fw" src="../Graphics/HiveMeta-Ex-Processor.png" height="314" width="562" /></p>

        <p class="p">With this configuration, Hadoop FS writes every record to the Hive table listed in the
            tag attribute and to the daily partition based on the time of processing.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_vh3_s4n_fw">
 <h3 class="title topictitle3">The Hive Metastore Destination</h3>

 
 <div class="body conbody"><p class="shortdesc">Now to process the metadata records - and to create and update tables in Hive - you need
        the Hive Metastore destination.</p>

  <p class="p">Connect the destination to the second output stream of the processor and configure the
            destination. The Hive Metastore just needs to know how to connect to Hive, so
            configuration of this destination is a breeze - all the work happens under the covers. </p>

        <p class="p">The destination connects to Hive the same way the processor does so your destination
            should look something like this, using the same Hive connection information:</p>

        <p class="p"><img class="image" id="concept_vh3_s4n_fw__image_imx_5pn_fw" src="../Graphics/HiveMeta-Ex-Dest.png" height="301" width="490" /></p>

        <p class="p">The magic here is: if the destination gets a metadata record that says you need a new
            table for a new web service, it creates the table with all the necessary columns so you
            can write the record (that triggered that metadata record) to the table.</p>

        <p class="p">And if the structure of the record going to a table changes, like adding a couple new
            fields, the destination updates the table in Hive so the record can be written to the
            table.</p>

        <p class="p">That covers the metadata, but what about the data?  </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_jzr_ypn_fw">
 <h3 class="title topictitle3">The Hadoop FS Destination</h3>

 
 <div class="body conbody"><p class="shortdesc">The Hadoop FS destination can write data to HDFS using record header attributes. If you
        wanted, you could use the MapR FS destination to write to MapR FS instead. Record header
        attributes contain the write details that the Hive Metadata processor generates and adds to
        records. </p>

  <p class="p">To write data to HDFS, you connect the Hadoop FS destination to the data output stream of the
            processor. You could connect the data output stream to the MapR FS destination to write
            data to MapR FS. The configuration options are essentially the same.... </p>

        <p class="p">When you configure the destination, instead of configuring a directory template, you
            configure the destination to use the directory in the record header. Instead of
            configuring an Avro schema, you indicate you have a schema in the record header. And you
            configure the destination to roll files when it sees a "roll" attribute in the record
            header. </p>

        <p class="p">The Output Files tab of the destination might look something like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_sbv_xrn_fw" src="../Graphics/HiveMeta-Ex-HDFS.png" height="466" width="508" /></p>

        <p class="p">And the Data Format tab looks like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_wzp_1sn_fw" src="../Graphics/HiveMeta-Ex-HDFS-Avro.png" height="327" width="478" /></p>

        <p class="p">With this configuration, the destination uses the information in record header attributes
            to write data to HDFS. It writes each record to the directory in the targetDirectory
            header attribute, using the Avro schema in the avroSchema header attribute. And it rolls
            a file when it spots the roll attribute in a record header. </p>

        <p class="p">Note that the destination can also use Max Records in File and Max Files Size to
            determine when to roll files.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_jlr_zlk_gw">
 <h3 class="title topictitle3">Processing Data</h3>

    
    <div class="body conbody"><p class="shortdesc">Now what happens when you start the pipeline?</p>

        <p class="p">This pipeline is set up to write data to different tables based on the table name in the
            "tag" attribute that was added to the record headers in the earlier pipeline. </p>

        <div class="p">Say the table names are "weblog" and "service". For each record with "weblog" as the tag
            attribute, the Hive Metadata processor evaluates the fields in the record as follows:
                <ul class="ul" id="concept_jlr_zlk_gw__ul_iml_1mk_gw">
                <li class="li">If the fields match the existing Hive table, it just writes the necessary
                    information into the targetDirectory and avroSchema stage attributes, and Hadoop
                    FS writes the record to the weblog table.</li>

                <li class="li">If a record includes a new field, the processor generates a metadata record that
                    the Hive Metastore destination uses to update the weblog table to include the
                    new column. It also writes information to stage attributes so Hadoop FS can
                    write the record to the updated weblog table.</li>

                <li class="li">If a record has missing fields, the processor just writes information to stage
                    attributes, and Hadoop FS writes the record to HDFS with null values for the
                    missing fields.</li>

                <li class="li">If a field has been renamed, the processor treats the field as a new field,
                    generating a metadata record that the Hive Metastore destination uses to update
                    the weblog table. When Hadoop FS writes the record, data is written to the new
                    field and a null value to the old field.</li>

                <li class="li">If a data type changes for an existing field, the processor treats the record as
                    an error record.</li>

            </ul>
</div>

        <p class="p">For each record with a "service" tag, the processor performs the same actions.</p>

        <div class="note note"><span class="notetitle">Note:</span> If a record includes a new tag value, the Hive Metadata processor generates a metadata
            record that the Hive Metastore destination uses to create a new table. And Hadoop FS
            writes the record to the new table. So if you spin up a new web service, you don't need
            to touch this pipeline to have it handle the new data set. </div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_ry2_qkm_hw">
 <h2 class="title topictitle2">Hive Data Types</h2>

 <div class="body conbody">
        <p class="p">The following table lists the <span class="ph">Data
                  Collector</span>
            data types and the corresponding Hive data types. The Hive Metadata processor uses these
            conversions when generating metadata records. The Hive Metadata destination uses these
            conversions when generating Hive CREATE TABLE and ALTER TABLE statements.</p>

  <div class="p">
            
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_ry2_qkm_hw__table_bcm_jlm_hw" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="50%" id="d131863e597"><span class="ph">Data
                  Collector</span> Data Type</th>

                            <th class="entry" valign="top" width="50%" id="d131863e602">Hive Data Type</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Boolean</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Boolean</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Byte</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Char</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Date</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Date </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Datetime</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Decimal</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Decimal</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Double</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Double</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Float</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Float</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Integer</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Int</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Long</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Bigint</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">List</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">List-Map</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Map</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Short</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">Int</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">String</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d131863e597 ">Time</td>

                            <td class="entry" valign="top" width="50%" headers="d131863e602 ">String</td>

                        </tr>

                    </tbody>

                </table>
</div>

        </div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" title="Event Framework"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Event Framework</span></a></span>  
<span class="navnext"><a class="link" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" title="Multithreaded Pipelines"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Multithreaded Pipelines</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>