<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cph_5h4_lx">
    <title>Event Framework Overview</title>
    <conbody>
        <p><indexterm>event framework<indexterm>overview</indexterm></indexterm>The <term>event
                framework</term> enables the use of <term>dataflow triggers</term> -  instructions
            for the pipeline to kick off asynchronous tasks in external systems in response to
            events that occur in the pipeline. With the event framework, you can configure dataflow
            triggers such as running a MapReduce job after the pipeline writes a file to HDFS. </p>
        <p>You can also use the event framework to store event information, such as when an origin
            starts or completes reading a file. </p>
        <p>Event handling begins with enabling event generation for a stage in the pipeline, then
            configuring the rest of the event stream to do your bidding. You can add an event stream
            to any pipeline that includes an event-generating stage. </p>
        <p>Event streams consist of the following conceptual components:<dl>
                <dlentry>
                    <dt>event generation</dt>
                    <dd>Events are generated by an event-generating stage when a specific action
                        takes place. The action that generates an event is related to how the stage
                        processes data and differs from stage to stage. </dd>
                    <dd>For example, the Hive Metastore destination updates the Hive metastore, so
                        it generates events each time it changes the metastore. In contrast, the
                        Hadoop FS destination writes files to HDFS, so it generates events each time
                        it closes a file. </dd>
                    <dd>When an event occurs, a stage generates an <term>event record</term> that
                        passes to the pipeline through an event output stream. Event streams cannot
                        be merged with data streams.</dd>
                </dlentry>
                <dlentry>
                    <dt>task execution</dt>
                    <dd>To use events to trigger a task, connect the event stream to an
                            <term>executor</term>. Executor stages perform tasks in external
                        systems.</dd>
                    <dd>Each time an executor receives an event record, it performs the specified
                        task.</dd>
                    <dd>For example, the Hive Query executor runs user-defined Hive or Impala
                        queries each time it receives an event. Similarly, the MapReduce executor
                        triggers a MapReduce job when it receives events. </dd>
                </dlentry>
                <dlentry>
                    <dt>event storage</dt>
                    <dd>To store event information, connect the event stream to a destination. The
                        destination writes the event records to the destination system, just like
                        any other data.</dd>
                    <dd>For example, you might store event records to keep an audit trail of the
                        files that the pipeline origin reads. </dd>
                </dlentry>
            </dl></p>
    </conbody>
</concept>
