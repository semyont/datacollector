<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_yym_xqt_5y">
 <title>What's New in 2.3.0.0</title>
 <conbody>
  <p><indexterm>what's new<indexterm>version 2.3.0.0</indexterm></indexterm><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            version 2.3.0.0 includes the following new features and enhancements:<dl>
                <dlentry>
                    <dt>Multithreaded Pipelines</dt>
                    <dd>You can use a multithreaded origin to generate <xref
                            href="../Multithreaded_Pipelines/MultithreadedPipelines_Overview.dita#concept_zpp_2xc_py"
                            >multithreaded pipelines</xref> to perform parallel processing. <p
                            dir="ltr">The new multithreaded framework includes the following
                            changes:</p><ul id="ul_slg_mrt_5y">
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Origins/HTTPServer.dita#concept_s2p_5hb_4y">HTTP
                                        Server origin</xref> - Listens on an HTTP endpoint and
                                    processes the contents of all authorized HTTP POST requests. Use
                                    the HTTP Server origin to receive high volumes of HTTP POST
                                    requests using multiple threads.</p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht"
                                        >Enhanced Dev Data Generator origin</xref> - Can create
                                    multiple threads for testing multithreaded pipelines.</p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Multithreaded_Pipelines/Monitoring.dita#concept_tdn_vwy_ry"
                                        >Enhanced runtime statistics</xref> - Monitoring a pipeline
                                    displays aggregated runtime statistics for all threads in the
                                    pipeline. You can also view the number of runners, i.e. threads
                                    and pipeline instances, being used.</p>
                            </li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>CDC/CRUD Enhancements</dt>
                    <dd>With this release, certain Data Collector stages enable you to easily
                        process change data capture (CDC) or transactional data in a pipeline. The
                        sdc.operation.type record header attribute is now used by all CDC-enabled
                        origins and CRUD-enabled stages:<p dir="ltr"><xref
                                href="../Pipeline_Design/CDC-Origins.dita#concept_iws_mhd_ty"
                                >CDC-enabled origins</xref>:</p><ul id="ul_vb4_prt_5y">
                            <li dir="ltr">
                                <p dir="ltr">The MongoDB Oplog and Salesforce origins are now
                                    enabled for processing changed data by including the CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>
                            </li>
                            <li dir="ltr">
                                <p>Though previously CDC-enabled, the Oracle CDC Client and JDBC
                                    Query Consumer for Microsoft SQL Server now include CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>
                                <p>Previous operation type header attributes are still supported for
                                    backward-compatibility. </p>
                            </li>
                        </ul><p dir="ltr"><xref
                                href="../Pipeline_Design/CRUDDestinations.dita#concept_lfb_phd_ty"
                                >CRUD-enabled stages</xref>:</p><ul id="ul_wb4_prt_5y">
                            <li dir="ltr">
                                <p dir="ltr">The JDBC Tee processor and JDBC Producer can now
                                    process changed data based on CRUD operations in record headers.
                                    The stages also include a default operation and unsupported
                                    operation handling. </p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr">The MongoDB and Elasticsearch destinations now look for
                                    the CRUD operation in the sdc.operation.type record header
                                    attribute. The Elasticsearch destination includes a default
                                    operation and unsupported operation handling.</p>
                            </li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Multitable Copy</dt>
                    <dd>You can use the new <xref
                            href="../Origins/MultiTableJDBCConsumer.dita#concept_zp3_wnw_4y">JDBC
                            Multitable Consumer origin</xref> when you need to copy multiple tables
                        to a destination system or for database replication. The JDBC Multitable
                        Consumer origin reads database data from multiple tables through a JDBC
                        connection. The origin generates SQL queries based on the table
                        configurations that you define.</dd>
                </dlentry>
                <dlentry>
                    <dt>Configuration</dt>
                    <dd>
                        <ul id="ul_up5_ntt_5y">
                            <li dir="ltr"><xref
                                    href="../Configuration/FileAuth-Configuring.dita#concept_wgy_rxt_5x"
                                    >Groups for file-based authentication</xref> - If you use
                                file-based authentication, you can now create groups of users when
                                multiple users use Data Collector. You configure groups in the
                                associated realm.properties file located in the Data Collector
                                configuration directory, $SDC_CONF. <p>If you use file-based
                                    authentication, you can also now view all user accounts granted
                                    access to the Data Collector, including the roles and groups
                                    assigned to each user.</p></li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Configuration/Authentication.dita#concept_dns_dvg_h5"
                                        >LDAP authentication enhancements</xref> - You can now
                                    configure Data Collector to use StartTLS to make secure
                                    connections to an LDAP server. You can also configure the
                                    userFilter property to define the LDAP user attribute used to
                                    log in to Data Collector. For example, a username, uid, or email
                                    address.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref href="../DPM/HTTPProxy.dita#concept_dmr_df5_5y">Proxy
                                        configuration for outbound requests</xref> - You can now
                                    configure Data Collector to use an authenticated HTTP proxy for
                                    outbound requests to Dataflow Performance Manager (DPM).</p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Configuration/JavaConfig_GarbageCollector.dita#concept_kqh_lj3_vx"
                                        >Java garbage collector logging</xref> - Data Collector now
                                    enables logging for the Java garbage collector by default. Logs
                                    are written to $SDC_LOG/gc.log. You can disable the logging if
                                    needed. </p>
                            </li>
                            <li dir="ltr">Heap dump for out of memory errors - Data Collector now
                                produces a heap dump file by default if it encounters an out of
                                memory error. You can configure the location of the heap dump file
                                or you can disable this default behavior. </li>
                            <li dir="ltr"><xref
                                    href="../Administration/ModifyingLogLevel.dita#task_lkv_g2f_wy"
                                    >Modifying the log level</xref> - You can now use the Data
                                Collector UI to modify the log level to display messages at another
                                severity level.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Pipelines</dt>
                    <dd>
                        <ul id="ul_lzx_xtt_5y">
                            <li dir="ltr">Pipeline renaming - You can now rename pipelines by
                                editing the Title general pipeline property.</li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                        >Field attributes</xref> - Data Collector now supports
                                    field-level attributes. Use the Expression Evaluator to add
                                    field attributes.</p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Origins</dt>
                    <dd>
                        <ul id="ul_ljw_15t_5y">
                            <li dir="ltr"><xref href="../Origins/HTTPServer.dita#concept_s2p_5hb_4y"
                                    >New HTTP Server origin</xref> - A multithreaded origin that
                                listens on an HTTP endpoint and processes the contents of all
                                authorized HTTP POST requests. Use the HTTP Server origin to read
                                high volumes of HTTP POST requests using multiple threads. </li>
                            <li dir="ltr">
                                <p><xref href="../Origins/HTTPtoKafka.dita#concept_izh_mqd_dy">New
                                        HTTP to Kafka origin</xref> - Listens on a HTTP endpoint and
                                    writes the contents of all authorized HTTP POST requests
                                    directly to Kafka. Use to read high volumes of HTTP POST
                                    requests and write them to Kafka. </p>
                            </li>
                            <li dir="ltr">
                                <p><xref href="../Origins/MapRDBJSON.dita#concept_ywh_k15_3y">New
                                        MapR DB JSON origin</xref> - Reads JSON documents from MapR
                                    DB JSON tables.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref href="../Origins/MongoDBOplog.dita#concept_mjn_yqw_4y">New
                                        MongoDB Oplog origin</xref> - Reads entries from a MongoDB
                                    Oplog. Use to process change information for data or database
                                    operations. </p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Origins/Directory-FileNamePattern-Mode.dita#concept_xd5_5z4_4y"
                                        >Directory origin enhancement</xref> - You can use regular
                                    expressions in addition to glob patterns to define the file name
                                    pattern to process files. </p>
                            </li>
                            <li dir="ltr">
                                <p><xref href="../Origins/HTTPClient-OAuth2.dita#concept_c13_zz1_5y"
                                        >HTTP Client origin enhancement</xref> - You can now
                                    configure the origin to use the OAuth 2 protocol to connect to
                                    an HTTP service.</p>
                            </li>
                            <li dir="ltr"><xref
                                    href="../Origins/JDBCConsumer.dita#concept_qhf_hjr_bs">JDBC
                                    Query Consumer origin enhancements</xref> - The JDBC Consumer
                                origin has been renamed to the JDBC Query Consumer origin. The
                                origin functions the same as in previous releases. It reads database
                                data using a user-defined SQL query through a JDBC connection. You
                                can also now configure the origin to enable auto-commit mode for the
                                JDBC connection and to disable validation of the SQL query.</li>
                            <li><xref href="../Origins/MongoDB.dita#concept_bk4_2rs_ns">MongoDB
                                    origin enhancements</xref> - You can now use a nested field as
                                the offset field. The origin supports reading the MongoDB BSON
                                timestamp for MongoDB versions 2.6 and later. And you can configure
                                the origin to connect to a single MongoDB server or node. </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Processors</dt>
                    <dd>
                        <ul id="ul_z35_cwt_5y">
                            <li dir="ltr">Field Type Converter processor enhancement - You can now
                                configure the processor to convert timestamp data in a Long field to
                                a String. Previously, you had to use one Field Type Converter
                                processor to convert the Long field to a Datetime, and then use
                                another processor to convert the Datetime field to a String.</li>
                            <li dir="ltr">
                                <p><xref href="../Processors/HTTPClient.dita#concept_ghx_ypr_fw"
                                        >HTTP Client processor enhancements</xref>  - You can now
                                    configure the processor to use the OAuth 2 protocol to connect
                                    to an HTTP service. You can also configure a rate limit for the
                                    processor, which defines the maximum number of requests to make
                                    per second.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref href="../Processors/JDBCLookup.dita#concept_ysc_ccy_hw"
                                        >JDBC Lookup processor enhancements</xref> - You can now
                                    configure the processor to enable auto-commit mode for the JDBC
                                    connection. You can also configure the processor to use a
                                    default value if the database does not return a lookup value for
                                    a column.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Processors/SalesforceLookup.dita#concept_k23_3rk_yx"
                                        >Salesforce Lookup processor enhancement</xref> - You can
                                    now configure the processor to use a default value if Salesforce
                                    does not return a lookup value for a field.</p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Processors/XMLParser.dita#concept_dtt_q5q_k5">XML
                                        Parser enhancement</xref> - A new Multiple Values Behavior
                                    property allows you to specify the behavior when you define a
                                    delimiter element and the document includes more than one value:
                                    Return the first value as a record, return one record with a
                                    list field for each value, or return all values as records.</p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Destinations</dt>
                    <dd>
                        <ul id="ul_uys_hwt_5y">
                            <li dir="ltr"><xref
                                    href="../Destinations/MapRDBJSON.dita#concept_i4h_2kj_dy">New
                                    MapR DB JSON destination</xref> - Writes data as JSON documents
                                to MapR DB JSON tables.</li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Destinations/DataLakeStore.dita#concept_jzm_kf4_zx"
                                        >Azure Data Lake Store destination</xref> enhancement - You
                                    can now use the destination in cluster batch pipelines. You can
                                    also process binary and protobuf data, use record header
                                    attributes to write records to files and roll files, and
                                    configure a file suffix and the maximum number of records that
                                    can be written to a file. </p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Destinations/Elasticsearch.dita#concept_u5t_vpv_4r"
                                        >Elasticsearch destination enhancement</xref> - The
                                    destination now uses the Elasticsearch HTTP API. With this API,
                                    the Elasticsearch version 5 stage library is compatible with all
                                    versions of Elasticsearch. Earlier stage library versions have
                                    been removed. Elasticsearch is no longer supported on Java 7.
                                    You’ll need to verify that Java 8 is installed on the Data
                                    Collector machine and remove this stage from the blacklist
                                    property in $SDC_CONF/sdc.properties before you can use it. </p>
                                <p>You can also now configure the destination to perform any of the
                                    following CRUD operations: create, update, delete, or index.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Destinations/HiveMetastore-EventRecords.dita#concept_x4p_fyc_rx"
                                        >Hive Metastore destination enhancement</xref> - New table
                                    events now include information about columns and partitions in
                                    the table.</p>
                            </li>
                            <li dir="ltr">
                                <p><xref
                                        href="../Destinations/HadoopFS-Recovery.dita#concept_uv2_vfb_vy"
                                        >Hadoop FS</xref>, <xref
                                        href="../Destinations/LocalFS-Recovery.dita#concept_s5n_ggc_vy"
                                        >Local FS</xref>, and <xref
                                        href="../Destinations/MapRFS-Recovery.dita#concept_vvr_ngc_vy"
                                        >MapR FS</xref> destination enhancement - The destinations
                                    now support recovery after an unexpected stop of the pipeline by
                                    renaming temporary files when the pipeline restarts.</p>
                            </li>
                            <li dir="ltr">Redis destination enhancement - You can now configure a
                                timeout for each key that the destination writes to Redis.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Executors</dt>
                    <dd>
                        <ul id="ul_zqk_4wt_5y">
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Executors/HiveQuery.dita#concept_kjw_llk_fx">Hive
                                        Query executor enhancements</xref>: <ul id="ul_gf2_qwt_5y">
                                        <li>The executor can now execute multiple queries for each
                                            event that it receives.</li>
                                        <li>It can also generate event records each time it
                                            processes a query.</li>
                                    </ul></p>
                            </li>
                        </ul>
                        <ul id="ul_brk_4wt_5y">
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Executors/JDBCQuery.dita#concept_j3r_gcv_sx">JDBC
                                        Query executor enhancement</xref> - You can now configure
                                    the executor to enable auto-commit mode for the JDBC
                                    connection.</p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Data Formats</dt>
                    <dd>
                        <ul id="ul_hpw_zwt_5y">
                            <li><xref
                                    href="../Pipeline_Design/WholeFile-TransferRate.dita#concept_prp_jzd_py"
                                    >Whole File enhancement</xref> - You can now specify a transfer
                                rate to help control the resources used to process whole files. You
                                can specify the rate limit in all origins that process whole
                                files.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Expression Language</dt>
                    <dd>
                        <ul id="ul_hq3_2xt_5y">
                            <li><xref
                                    href="../Expression_Language/PipelineFunctions.dita#concept_dvg_nqn_wx"
                                    >New pipeline functions</xref> - You can use the following new
                                pipeline functions to return pipeline information:<ul
                                    id="ul_mls_bp1_cz">
                                    <li>pipeline:id() - Returns the pipeline ID, a UUID that is
                                        automatically generated and used by Data Collector to
                                        identify the pipeline. <note>The existing pipeline:name()
                                            function now returns the pipeline ID instead of the
                                            pipeline name since pipeline ID is the correct way to
                                            identify a pipeline.</note></li>
                                    <li>
                                        <p>pipeline:title() - Returns the pipeline title or
                                            name.</p>
                                    </li>
                                </ul></li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Expression_Language/RecordFunctions.dita#concept_p1z_ggv_1r"
                                        >New record functions</xref> - You can use the following new
                                    record functions to work with field attributes:<ul
                                        id="ul_k4h_kxt_5y">
                                        <li>record:fieldAttribute (&lt;field path>, &lt;attribute
                                            name>) - Returns the value of the specified field
                                            attribute. </li>
                                        <li>record:fieldAttributeOrDefault (&lt;field path>,
                                            &lt;attribute name>, &lt;default value>) - Returns the
                                            value of the specified field attribute. Returns the
                                            default value if the attribute does not exist or
                                            contains no value.</li>
                                    </ul></p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Expression_Language/StringFunctions.dita#concept_ahp_f4v_1r"
                                        >New string functions</xref> - You can use the following new
                                    string functions to transform string data: <ul
                                        id="ul_knh_mxt_5y">
                                        <li>str:urlEncode (&lt;string>, &lt;encoding>) - Returns a
                                            URL encoded string from a decoded string using the
                                            specified encoding format. </li>
                                        <li>str:urlDecode (&lt;string>, &lt;encoding>) - Returns a
                                            decoded string from a URL encoded string using the
                                            specified encoding format.</li>
                                    </ul></p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Expression_Language/TimeFunctions.dita#concept_qkr_trf_sw"
                                        >New time functions</xref> - You can use the following new
                                    time functions to transform datetime data: <ul
                                        id="ul_yk5_pxt_5y">
                                        <li>time:dateTimeToMilliseconds (&lt;Date object>) -
                                            Converts a Date object to an epoch or UNIX time in
                                            milliseconds. </li>
                                        <li>time:extractDateFromString(&lt;string>, &lt;format
                                            string>) - Extracts a Date object from a String, based
                                            on the specified date format. </li>
                                        <li>time:extractStringFromDateTZ (&lt;Date object>,
                                            &lt;timezone>, &lt;format string>) - Extracts a string
                                            value from a Date object based on the specified date
                                            format and time zone.</li>
                                    </ul></p>
                            </li>
                            <li dir="ltr">
                                <p dir="ltr"><xref
                                        href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                        >New and enhanced miscellaneous functions</xref> - You can
                                    use the following new and enhanced miscellaneous functions: <ul
                                        id="ul_m2x_nxt_5y">
                                        <li>offset:column(&lt;position>) - Returns the value of the
                                            positioned offset column for the current table.
                                            Available only in the additional offset column
                                            conditions of the JDBC Multitable Consumer origin. </li>
                                        <li>every function - You can now use the function with the
                                            hh() datetime variable in directory templates. This
                                            allows you to create directories based on the specified
                                            interval for hours.</li>
                                    </ul></p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>
