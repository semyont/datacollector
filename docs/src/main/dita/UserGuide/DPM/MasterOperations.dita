<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_xvp_x2x_fz">
 <title>Master Dataflow Operations</title>
 <shortdesc>As a DevOps or site reliability engineer, you can master your day-to-day operations by
        defining data SLAs (service level agreements) to ensure that incoming data meets business
        requirements for availability and accuracy.</shortdesc>
 <conbody>
  <p>
            <draft-comment author="alisontaylor">text copied from same topic in DPM User Guide. Make
                the same updates in both places. </draft-comment>
        </p>
        <p>In addition to measuring the health of a topology, you define data SLAs to define the
            expected thresholds of the data throughput rate or the error record rate. Data SLAs
            trigger an alert when the specified threshold is reached. Data SLA alerts provide
            immediate feedback on the data processing rates expected by your team. They enable you
            to master your dataflow operations by quickly investigating and resolving issues that
            arise.</p>
        <p>For example, you have service level agreements with the operational analytics team to
            ensure that all of the data captured and processed in the Customer 360 topology is clean
            and available for immediate analysis. If any of the Customer 360 jobs encounter
            processing errors, you must immediately resolve those issues. You define and activate a
            data SLA that triggers an alert when a job in the topology encounters more than 100
            error records per second. </p>
        <p>If the alert triggers, DPM notifies you with a red Notifications icon in the top toolbar:
                <image href="../Graphics/icon_Notifications.png" scale="70" id="image_gk1_rfx_fz"/>.
            You drill into the details of the data SLA to discover which threshold was reached and
            to investigate the issues that need to be resolved. The triggered data SLA displays a
            graph of the error record rate. The red line in the graph represents the defined
            threshold, as follows:</p>
        <p><image href="../Graphics/DPM_MasterOperations.png" scale="55" id="image_xyw_rfx_fz"/></p>
        <p>We've seen how you can use <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> to
            turn a high-level architecture diagram of your dataflows into pipelines and jobs that
            you can then manage and measure from a single topology. Give it a try, and see for
            yourself how easy <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> has
            made it to map, measure, and master your data in motion.</p>
 </conbody>
</concept>
