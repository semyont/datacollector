<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_lfl_dvd_1y">
 <title>Developing the Spark Application</title>
 <conbody>
  <p><indexterm>Spark Evaluator processor<indexterm>writing the
                application</indexterm></indexterm><indexterm>Spark
                    application<indexterm>custom</indexterm></indexterm>To develop a custom Spark
            application, you write the Spark application and then package a JAR file containing the
            application.</p>
        <p>Use Java or Scala to write a custom Spark class that implements the StreamSets
            SparkTransformer API: <xref
                href="https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api"
                format="html" scope="external"/>.</p>
        <p>You can include the following methods in the custom class:</p>
        <dl>
            <dlentry>
                <dt>init</dt>
                <dd>Optional. The <codeph>init</codeph> method is called once when the pipeline
                    starts to read arguments that you configure in the Spark Evaluator processor.
                    Use the <codeph>init</codeph> method to make connections to external systems or
                    to read configuration details or existing data from external systems.</dd>
            </dlentry>
            <dlentry>
                <dt>transform</dt>
                <dd>Required. The <codeph>transform</codeph> method is called for each batch of
                    records that the pipeline processes. <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> passes a batch of data to the <codeph>transform</codeph> method as a
                    Resilient Distributed Dataset (RDD). The method processes the data according to
                    the custom code.</dd>
            </dlentry>
            <dlentry>
                <dt>destroy</dt>
                <dd>Optional. If you include an <codeph>init</codeph> method that makes connections
                    to external systems, you should also include the <codeph>destroy</codeph> method
                    to close the connections. The <codeph>destroy</codeph> method is called when the
                    pipeline stops.</dd>
            </dlentry>
        </dl>
        <p>When you finish writing the custom Spark class, package a JAR file containing the Spark
            application. Compile against the same stage library version that you use for the Spark
            Evaluator processor. For example, if you are using the Spark Evaluator processor
            included in the stage library for the Cloudera CDH version 5.9 distribution of Hadoop,
            build the application against Spark integrated into Cloudera CDH version 5.9.</p>
        <p>If you used Scala to write the custom Spark class, you must build the application so that
            it is compatible with Scala 2.10.</p>
 </conbody>
</concept>
