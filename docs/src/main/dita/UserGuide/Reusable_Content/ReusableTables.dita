<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_wfr_rnw_yq">
 <title>Reusable Tables of Information</title>
 <shortdesc/>
 <conbody>
  <p>
   <draft-comment author="Loretta">The following IgnoreControlChar-row is used in Configuring a -
    Directory, File Tail, Kafka Consumer, Kinesis Consumer, JSON Parser, Log
    Parser:]</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_mxl_xrm_js">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row id="IgnoreControlChars-row">
       <entry>Ignore Ctrl Characters <xref href="../Pipeline_Design/ControlCharacters.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xwx_xrm_js"/></xref></entry>
       <entry>Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Origin - first step rows for event handling
    origins</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_f5l_np2_px">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>General Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="O-1stStep-Name">
       <entry>Name</entry>
       <entry>Stage name.</entry>
      </row>
      <row id="O-1stStep-Desc">
       <entry>Description</entry>
       <entry>Optional description.</entry>
      </row>
      <row>
       <entry>Produce Events </entry>
       <entry id="O-1stStep-entry-Events">Generates event records when events occur. Use for event
        handling. <xref href="../Event_Handling/EventFramework-Overview.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_plp_tp2_px"
        /></xref></entry>
      </row>
      <row id="O-1stStep-Error">
       <entry>On Record Error <xref href="../Pipeline_Design/ErrorHandling-Stage.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_g5l_np2_px"
        /></xref></entry>
       <entry>Error record handling for the stage: <ul id="ul_h5l_np2_px">
         <li>Discard - Discards the record.</li>
         <li>Send to Error - Sends the record to the pipeline for error handling.</li>
         <li>Stop Pipeline - Stops the pipeline. </li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">ORIGIN rows - <uicontrol>ProduceSingleRec</uicontrol> is used by
    Kafka Consumer and MapR Streams Consumer. <uicontrol>MaxBatchSize</uicontrol> and
     <uicontrol>BatchWaitTime</uicontrol> rows are used in Configuring Kafka Consumer, JMS Consumer,
    Salesforce origin, Salesforce Lookup. See if they should go anywhere else.</draft-comment>
   <draft-comment author="Loretta">Charsets are used by message and file origins. But Directory
    Charset is standalone. </draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_tft_4jk_dt">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry/>
       <entry/>
      </row>
      <row id="ProduceSingleRec">
       <entry>Produce Single Record</entry>
       <entry>For each partition, generates a single record for records that include multiple
        objects. <p>When not selected, the origin generates multiple records when a record includes
         multiple objects.</p></entry>
      </row>
      <row id="MaxBatchSize">
       <entry>Max Batch Size (records)</entry>
       <entry>Maximum number of records processed at one time. Honors values up to the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> maximum
        batch size. <p>Default is 1000. The <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> default
         is 1000.</p></entry>
      </row>
      <row id="BatchWaitTime">
       <entry>Batch Wait Time (ms) <xref href="../Origins/BatchSizeWaitTime.dita#concept_ypd_vgr_5q">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_mgp_2q3_br"
          placement="inline"/></xref></entry>
       <entry>Number of milliseconds to wait before sending a partial or empty batch. </entry>
      </row>
      <row id="MessagesCharset">
       <entry>Charset</entry>
       <entry>Character encoding of the messages to be processed.</entry>
      </row>
      <row id="Charset">
       <entry>Charset</entry>
       <entry>Character encoding of the files to be processed.</entry>
      </row>
      <row id="row-CharsetData">
       <entry>Charset</entry>
       <entry>Character encoding of the data to be processed.</entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following rows are used in the Data Collector Console -
    Overview, </draft-comment>
  </p>
  <simpletable>
   <strow id="Icon-Help">
    <stentry><image href="../Graphics/icon_OverCHelp.png" id="image_bkz_wk3_ts"/></stentry>
    <stentry>Help icon</stentry>
    <stentry>Provides context-sensitive help based on the information in the panel. </stentry>
   </strow>
  </simpletable>
  <p>
   <draft-comment author="Loretta">The following row is used in Configuring Hive
    Streaming</draft-comment>
  </p>
  <table frame="all" rowsep="1" colsep="1" id="table_ps1_hln_jt">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry/>
      <entry/>
     </row>
    </thead>
    <tbody>
     <row id="FIELD2ColumnMapping">
      <entry>Field to Column Mapping</entry>
      <entry>
       <p>Use to override the default field to column mappings. </p>
       <p>By default, fields are written to columns of the same name. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS destinations:
    Amazon S3, Kinesis Firehose, Kinesis Producer. And by writing aggregated statistics to Kinesis,
    in DPM chapter > Aggregated Statistics for Pipelines > Configuring a Pipeline to Aggregate
    Statistics</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vnv_ncr_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID </entry>
       <entry id="AWSDest_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSDest_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS origins:
    Amazon S3 and Kinesis Consumer.</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_lnx_51w_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID</entry>
       <entry id="AWSOrigin_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSOrigin_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="Loretta">The following rows are used in full for the Amazon S3 origin. The
    Amazon S3 destination and Kinesis Firehose destination reuse individual rows. Data Format, File
    Compression, and File Name Pattern within Compressed Directory are also used in Directory, File
    Tail, HTTP Client, and SFTP Client.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="AmazonS3-oProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>File Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="S3-Region">
       <entry>Region</entry>
       <entry>Amazon S3 region. </entry>
      </row>
      <row id="S3-Endpoint">
       <entry>Endpoint</entry>
       <entry>Endpoint to connect to when you select Other for the region. Enter the endpoint
        name.</entry>
      </row>
      <row id="S3Bucket">
       <entry>Bucket</entry>
       <entry>Bucket that contains the objects to be read.</entry>
      </row>
      <row id="S3Folder">
       <entry>Common Prefix <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wm3_212_wv"/>
        </xref></entry>
       <entry>Optional common prefix that describes the location of the objects. When defined, the
        common prefix acts as a root for the prefix pattern.</entry>
      </row>
      <row id="S3ObjectPathDelimiter">
       <entry>Delimiter</entry>
       <entry>Delimiter used by Amazon S3 to define the prefix hierarchy.<p>Default is slash ( /
         ).</p></entry>
      </row>
      <row id="S3IncludeMetadata-row">
       <entry>Include Metadata <xref href="../Origins/AmazonS3-RecordHeaderAttrs.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_sw3_wjx_yw"/>
        </xref></entry>
       <entry>Includes system-defined and user-defined metadata in record header attributes.
       </entry>
      </row>
      <row id="S3FileNamePattern">
       <entry>Prefix Pattern <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_tfr_bk5_ht"/>
        </xref></entry>
       <entry>
        Prefix pattern that describes the objects to be processed.
        <p>You can include the entire path to the objects. You can also use Ant-style path patterns
         to read objects recursively. </p>
       </entry>
      </row>
      <row id="S3BufferLimit">
       <entry>Buffer Limit (KB)</entry>
       <entry>Maximum buffer size. The buffer size determines the size of the record that can be
        processed. <p>Decrease when memory on the <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> machine
         is limited. Increase to process larger records when memory is available. </p><p>Default is
         128 KB.</p></entry>
      </row>
      <row id="Origin-FileCompression">
       <entry>Compression Format <xref href="../Origins/FileCompressionFormats.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xqq_yv4_c5"/></xref></entry>
       <entry>The compression format of the files:<ul id="ul_vph_jp2_qs">
         <li>None - Processes only uncompressed files.</li>
         <li>Compressed File - Processes files compressed by the supported compression formats.</li>
         <li>Archive - Processes files archived by the supported archive formats.</li>
         <li>Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>
        </ul></entry>
      </row>
      <row id="Origin-FilePatternCompressed">
       <entry>File Name Pattern within Compressed Directory</entry>
       <entry>File name pattern that represents the files to process within the compressed
        directory. You can use UNIX-style wildcards, such as an asterisk or question mark. For
        example, *.json.<p>Default is *, which processes all files.</p></entry>
      </row>
      <row id="S3DataFormat">
       <entry>Data Format <xref href="../Origins/AmazonS3-DataFormat.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_w4w_q3p_ht"/>
        </xref></entry>
       <entry id="entryDataFormats">Data format for source files. Use one of the following formats:<ul id="ul_y1t_wql_5q">
         <li>Avro</li>
         <li>Delimited</li>
         <li>JSON</li>
         <li>Log</li>
         <li>Protobuf</li>
         <li>SDC Record <xref href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
           <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wjh_ycl_br"
            placement="inline"/></xref></li>
         <li>Text</li>
         <li>Whole File <xref href="../Pipeline_Design/WholeFile.dita">
           <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
            id="image_igx_zzm_zw"/></xref></li>
         <li>XML</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor">Kinesis Consumer, Kinesis Producer and DPM chapter >
   Aggregated Statistics for Pipelines > Configuring a Pipeline to Aggregate Statistics use these
   rows</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_kqy_mw1_fx">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <tbody>
     <row id="rowKinesisRegion">
      <entry>Region</entry>
      <entry>Amazon Web Services region that hosts the Kinesis cluster.</entry>
     </row>
     <row id="rowKinesisEndpoint">
      <entry>Endpoint</entry>
      <entry>Endpoint to connect to when you select Other for the region. Enter the endpoint
       name.</entry>
     </row>
     <row id="rowKinesisStreamName">
      <entry>Stream Name</entry>
      <entry>Kinesis stream name.</entry>
     </row>
     <row id="rowKinesisPartitionStrategy">
      <entry>Partitioning Strategy</entry>
      <entry>Strategy to write data to Kinesis shards:<ul id="ul_nqh_qw1_fx">
        <li>Random - Generates a random partition key.</li>
        <li>
         <p>Expression - Uses the result of an expression as the partition key.</p>
        </li>
       </ul></entry>
     </row>
     <row id="rowKinesisPartitionExp">
      <entry>Partition Expression</entry>
      <entry>Expression to generate the partition key used to pass data to different shards. <p>Use
        for the expression partition strategy. </p></entry>
     </row>
     <row id="rowKinesisConfig">
      <entry>Kinesis Producer Configuration</entry>
      <entry>Additional Kinesis properties. <p>When you add a configuration property, enter the
        exact property name and the value. The Kinesis Producer does not validate the property names
        or values. </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">HTTP Client origin and processor rows:</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_jf4_g24_jw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="HTTP-ReqTransferEncod">
       <entry>Request Transfer Encoding</entry>
       <entry>Use one of the following encoding types:<ul id="ul_o3l_mgv_vw">
         <li>Buffered - The standard transfer encoding type. </li>
         <li>Chunked - Transfers data in chunks. Not supported by all servers.</li>
        </ul><p>The default is Chunked.</p></entry>
      </row>
      <row id="HTTP-ConnectTimeout">
       <entry>Connect Timeout</entry>
       <entry>Maximum number of milliseconds to wait for a connection. <p>Use 0 to wait
         indefinitely.</p></entry>
      </row>
      <row id="HTTP-ReadTimeout">
       <entry>Read Timeout</entry>
       <entry>Maximum number of milliseconds to wait for data. <p>Use 0 to wait
        indefinitely.</p></entry>
      </row>
      <row id="HTTP-AuthType">
       <entry>Authentication Type</entry>
       <entry>Determines the authentication type used to connect to the server:<ul
         id="ul_icm_h1l_35">
         <li>None - Performs no authentication.</li>
         <li>Basic - Uses basic authentication. Requires a username and password. <p>Use with HTTPS
           to avoid passing unencrypted credentials.</p></li>
         <li>Digest - Uses digest authentication. Requires a username and password.</li>
         <li>Universal - Makes an anonymous connection, then provides authentication credentials
          upon receiving a 401 status and a WWW-Authenticate header request. <p>Requires a username
           and password associated with basic or digest authentication.</p><p>Use only with servers
           that respond to this workflow.</p></li>
         <li>OAuth - Uses OAuth 1.0 authentication. Requires OAuth credentials.</li>
        </ul></entry>
      </row>
      <row id="HTTP-OAuth2">
       <entry>Use OAuth 2</entry>
       <entry>Enables using OAuth 2 authorization to request access tokens. <p>You can use OAuth 2
         authorization with none, basic, digest, or universal authentication.</p></entry>
      </row>
      <row id="HTTP-UseProxy">
       <entry>Use Proxy</entry>
       <entry>
        <p>Enables using an HTTP proxy to connect to the system. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor">OAuth 2 table for HTTP Client origin and
   processor</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="HTTP_OAuth2">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>OAuth 2 Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Credentials Grant Type</entry>
      <entry>Type of client credentials grant type required by the HTTP service:<ul
        id="ul_m3c_t1f_5y">
        <li>Client credentials grant</li>
        <li>Resource owner password credentials grant</li>
        <li>JSON Web Tokens (JWT)</li>
       </ul></entry>
     </row>
     <row>
      <entry>Token URL</entry>
      <entry>URL to request the access token.</entry>
     </row>
     <row>
      <entry>Client ID</entry>
      <entry>Client ID that the HTTP service uses to identify the HTTP client.<p>For client
        credentials grant that uses a client ID and secret for authentication. Or, for resource
        owner password credentials grant that requires a client ID and secret.</p></entry>
     </row>
     <row>
      <entry>Client Secret</entry>
      <entry>Client secret that the HTTP service uses to authenticate the HTTP client.<p>For client
        credentials grant that uses a client ID and secret for authentication. Or, for resource
        owner password credentials grant that requires a client ID and secret.</p><note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Tip_ClientID"/></entry>
     </row>
     <row>
      <entry>User Name</entry>
      <entry>Resource owner user name.<p>For resource owner password credentials grant.</p></entry>
     </row>
     <row>
      <entry>Password</entry>
      <entry>Resource owner password.<p>For resource owner password credentials grant.</p><note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Tip_Usernames"/></entry>
     </row>
     <row>
      <entry>JWT Signing Algorithm</entry>
      <entry>Algorithm used to sign the JSON Web Token (JWT).<p>Default is none. For JSON Web Tokens
        grant.</p></entry>
     </row>
     <row>
      <entry>JWT Signing Key</entry>
      <entry>Base64 encoded key used to sign the JSON Web Token, if you selected a signing
        algorithm.<note conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Tip_JWTKey"/><p>For JSON Web
        Tokens grant.</p></entry>
     </row>
     <row>
      <entry>JWT Claims</entry>
      <entry>Claims to use in the JSON Web Token request, entered in JSON format. Enter each claim
       required by the HTTP service. You can include the expression language in the JWT
        claims.<p>For example, to read from Google service accounts, enter the following claims with
        the appropriate
        values:</p><codeblock>{
  "iss":"my_name@my_account.iam.gserviceaccount.com",
  "scope":"https://www.googleapis.com/auth/drive",
  "aud":"https://www.googleapis.com/oauth2/v4/token",
  "exp":${(time:dateTimeToMilliseconds(time:now())/1000) + 50 * 60},
  "iat":${time:dateTimeToMilliseconds(time:now())/1000}
}</codeblock><p>For
        JSON Web Tokens grant.</p></entry>
     </row>
     <row>
      <entry>Request Transfer Encoding</entry>
      <entry>Form of encoding to use when the stage requests an access token: buffered or
        chunked.<p>Default is buffered.</p></entry>
     </row>
     <row>
      <entry>Additional Key-Value Pairs</entry>
      <entry>Optional key-value pairs to send to the token URL when requesting an access token. For
       example, you can define the OAuth 2 <codeph>scope</codeph> request parameter.<p>Use the
         <uicontrol>Add</uicontrol> icon to add additional key-value pairs.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">PROCESSOR rows</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_u2r_4x5_lv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.0*"/>
     <colspec colname="c2" colnum="2" colwidth="1.0*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="P-HashType">
       <entry>Hash Type</entry>
       <entry>Algorithm to use to hash field values:<ul id="ul_kmd_rnk_wq">
         <li>MD5 - Produces a 128-bit (16-byte) hash value, typically expressed in text format as a
          32 digit hexadecimal number.</li>
         <li>SHA1 - Produces a 160-bit (20-byte) hash value.</li>
         <li>SHA2 - Based on SHA1, but uses a set of four hash functions: 224, 256, 384, or 512
          bits.</li>
         <li>MURMUR3_128 - Produces a 128-bit (16 byte) hash value.</li>
        </ul></entry>
      </row>
      <row id="P-HashTargetField">
       <entry>Target Field</entry>
       <entry>Field in the record to use for hashed data. If the field does not exist, Field Hasher
        creates the field. </entry>
      </row>
      <row id="P-HashHeaderAtt">
       <entry>Header Attribute</entry>
       <entry>Attribute in the record header to use for hashed data. If the attribute does not
        exist, Field Hasher creates the attribute.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">The Hive Metadata processor and the Hive Metastore destination
   configuring topics use these individual rows and the Max entries description. Hive Query executor
   also uses rows.</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_w1l_34y_dw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Hive Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="P-D_HiveJDBCURL">
       <entry>JDBC URL</entry>
       <entry>JDBC URL for Hive. You can use the default, or replace the expression for the database
        name with a specific database name when appropriate.</entry>
      </row>
      <row id="P-D_HiveJDBCdriver">
       <entry>JDBC Driver Name</entry>
       <entry>The fully-qualified JDBC driver name.</entry>
      </row>
      <row id="P-D_HiveConfigDir">
       <entry>Hadoop Configuration Directory</entry>
       <entry>
        <p>Absolute path to the directory containing the Hive and Hadoop configuration files. For a
         Cloudera Manager installation, enter hive-conf. </p>
        <p>The stage uses the following configuration files: <ul
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HiveStreamingFiles"
          id="ul_tqf_lms_dw">
          <li/>
         </ul></p>
        <note>Properties in the configuration files are overridden by individual properties defined
         in this stage. </note>
       </entry>
      </row>
      <row id="P-D-Hive-AddConfig">
       <entry>Additional Hadoop Configuration</entry>
       <entry>
        <p>Additional properties to use. </p>
        <p>To add properties, click <uicontrol>Add</uicontrol> and define the property name and
         value. Use the property names and values as expected by HDFS and Hive.</p>
       </entry>
      </row>
      <row>
       <entry>Max Cache Size (not conrefed)</entry>
       <entry id="P-D_HiveMaxCacheSize">Maximum number of entries in the cache. <p>When the cache
         reaches the maximum size, the oldest cached entries are evicted to allow for new
         data.</p><p>Default is -1, an unlimited cache size.</p></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">Event destinations and executors, 1st step. Rows
   used.</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_wsl_2rx_mx">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>General Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="D-1stStep-Name">
       <entry>Name</entry>
       <entry>Stage name.</entry>
      </row>
      <row id="D-1stStep-Desc">
       <entry>Description</entry>
       <entry>Optional description.</entry>
      </row>
      <row id="D-1stStep-Library">
       <entry>Stage Library</entry>
       <entry>Library version that you want to use. </entry>
      </row>
      <row>
       <entry>Produce Events <xref href="../Event_Handling/EventGenerationStages.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wrq_mrx_mx"
        /></xref></entry>
       <entry id="D-1stStep-entry-Events">Generates event records when events occur. Use for event
        handling. <xref href="../Event_Handling/EventFramework-Overview.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_l15_ft2_px"
        /></xref></entry>
      </row>
      <row id="D-1stStep-ReqFields">
       <entry>Required Fields <xref href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_xsl_2rx_mx"
        /></xref></entry>
       <entry>Fields that must include data to be passed into the stage. <note outputclass=""
         type="tip">You might include fields that the stage uses.</note></entry>
      </row>
      <row id="D-1stStep-Precond">
       <entry>Preconditions <xref href="../Pipeline_Design/Preconditions.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_ysl_2rx_mx"
        /></xref></entry>
       <entry>Conditions that must evaluate to TRUE to allow a record to enter the stage for
        processing. Click <uicontrol>Add</uicontrol> to create additional preconditions. </entry>
      </row>
      <row id="D-1stStep-Error">
       <entry>On Record Error <xref href="../Pipeline_Design/ErrorHandling-Stage.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_zsl_2rx_mx"
        /></xref></entry>
       <entry>Error record handling for the stage: <ul id="ul_atl_2rx_mx">
         <li>Discard - Discards the record.</li>
         <li>Send to Error - Sends the record to the pipeline for error handling.</li>
         <li>Stop Pipeline - Stops the pipeline.</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Hadoop FS, MapR FS, Local FS use
     <uicontrol>D-Event-RHattributes</uicontrol> in Event Records. All others use the creation_date
    entry: <uicontrol>entry-Event-creationDate</uicontrol>. </draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="D-Event-RHattributes">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Record Header Attribute</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry id="entry-eventType">sdc.event.type</entry>
       <entry>Event type. Uses one of the following types:<ul id="ul_m12_mgp_qx">
         <li>file-closed - Generated when the destination closes a file.</li>
         <li>wholeFileProcessed - Generated when the destination completes streaming a whole
          file.</li>
        </ul></entry>
      </row>
      <row id="row-eventVersion">
       <entry>sdc.event.version</entry>
       <entry>An integer that indicates the version of the event record type.</entry>
      </row>
      <row id="row-eventTimestamp">
       <entry>sdc.event.creation_timestamp</entry>
       <entry id="entry-Event-creationDate">Epoch timestamp when the stage created the event.
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Hadoop FS, Local FS, MapR FS</draft-comment>
  </p>
  <p/>
  <p>
   <draft-comment author="Loretta">DESTINATION rows</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_l3k_ksh_r5">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="D-CHARSET-file">
       <entry>Charset</entry>
       <entry>Character set to use when writing files. </entry>
      </row>
      <row id="D-CHARSET-other">
       <entry>Charset</entry>
       <entry>Character set to use when writing data. </entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="alisontaylor"><b>table-JDBCProps</b> - JDBC Query Consumer, Multi Table
    JDBC Consumer, JDBC Lookup, JDBC Producer, and JDBC Tee use most rows in this table. Oracle CDC
    is using a couple too. JDBC Query executor also uses some. – no one uses the whole table AFAI
    can tell.&lt;lc></draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vwl_qcq_tw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.0*"/>
     <thead>
      <row>
       <entry>JDBC Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="JDBCConnectString_row">
       <entry>JDBC Connection String</entry>
       <entry>Connection string to use to connect to the database.<p>Some databases, such as
         Postgres, require the schema in the connection string. Use the connection string format
         required by the database.</p></entry>
      </row>
      <row id="FieldToColumnJDBC_row">
       <entry>Field to Column Mapping</entry>
       <entry>Use to override the default field to column mappings. By default, fields are written
        to columns of the same name.<p>When you override the mappings, you can define parameterized
         values to apply SQL functions to the field values before writing them to columns. For
         example, to convert a field value to an integer, enter the following for the parameterized
         value:<codeblock>CAST(? AS INTEGER)</codeblock></p><p>The question mark (?) is substituted
         with the value of the field. Leave the default value of ? if you do not need to apply a SQL
         function.</p><p>Use the <uicontrol>Add</uicontrol> icon to create additional field to
         column mappings.</p></entry>
      </row>
      <row id="JDBCQueryInterval">
       <entry>Query Interval</entry>
       <entry>Amount of time to wait between queries. Enter an expression based on a unit of time.
        You can use SECONDS, MINUTES, or HOURS.<p>Default is 10 seconds: ${10 * SECONDS}.
        </p></entry>
      </row>
      <row id="JDBCUseCredentials_row">
       <entry>Use Credentials</entry>
       <entry>Enables entering credentials on the Credentials tab. Use when you do not include
        credentials in the JDBC connection string.</entry>
      </row>
      <row id="JDBCMaxBatch_row">
       <entry>Max Batch Size (records)</entry>
       <entry>Maximum number of records to include in a batch.</entry>
      </row>
      <row id="JDBCMaxClob_row">
       <entry>Max Clob Size (characters)</entry>
       <entry>Maximum number of characters to be read in a Clob field. Larger data is
        truncated.</entry>
      </row>
      <row id="JDBCMaxBlob_row">
       <entry>Max Blob Size (bytes)</entry>
       <entry>
        <p>Maximum number of bytes to be read in a Blob field. </p>
       </entry>
      </row>
      <row id="LogFormat_row">
       <entry>Change Log Format</entry>
       <entry>Format of change capture data. Use when processing change capture data. </entry>
      </row>
      <row>
       <entry>Default Operation </entry>
       <entry>&lt;&lt; using conref to elasticsearch>></entry>
      </row>
      <row>
       <entry>Unsupported Operation Handling  </entry>
       <entry>&lt;&lt; using conref to elasticsearch>></entry>
      </row>
      <row id="RollbackBatch_row">
       <entry>Rollback Batch On Error</entry>
       <entry>Rolls back the entire batch when an error occurs within the batch. </entry>
      </row>
      <row id="MultiRowInsert_row">
       <entry>Use Multi-Row Insert</entry>
       <entry>Determines how the stage inserts records. Select to allow inserts of multiple rows at
        a time. Clear to insert a single row at a time. </entry>
      </row>
      <row id="ParamLimit_row">
       <entry>Statement Parameter Limit</entry>
       <entry>Defines the number of parameters allowed in the prepared statement for multi-row
         inserts.<p>Use -1 to disable the parameter limit. Default is -1.</p></entry>
      </row>
      <row id="JDBCAddtitionalProps_row">
       <entry>Additional JDBC Configuration Properties</entry>
       <entry>Additional JDBC configuration properties to use. To add properties, click
         <uicontrol>Add</uicontrol> and define the JDBC property name and value. <p>Use the property
         names and values as expected by JDBC. </p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="Loretta"><b>table-JDBCAdvProps</b> - JDBC Lookup uses the whole table.
    JDBC Consumer uses rows but has its own. JDBC Tee uses some rows. Producer uses a bunch of rows.
    Oracle CDC uses all rows but Auto Commit. – note updated LC 2/9/17</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table-JDBCAdvProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Advanced Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="row-MaxPoolSize">
       <entry>Maximum Pool Size </entry>
       <entry>The maximum number of connections to create. <p>Default is 1. The recommended value is
         1.</p></entry>
      </row>
      <row id="row-MaxIdleConn">
       <entry>Minimum Idle Connections</entry>
       <entry>The minimum number of connections to create and maintain. To define a fixed connection
        pool, set to the same value as Maximum Pool Size. <p>Default is 1. </p></entry>
      </row>
      <row id="row-ConTimeout">
       <entry>Connection Timeout</entry>
       <entry>Maximum time to wait for a connection. Use a time constant in an expression to define
        the time increment. <p>Default is 30 seconds, defined as follows:
         <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-IdleTimeout">
       <entry>Idle Timeout</entry>
       <entry>Maximum time to allow a connection to idle. Use a time constant in an expression to
        define the time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is
         30 minutes, defined as follows: <codeblock>${30 * MINUTES}</codeblock></p></entry>
      </row>
      <row id="row-MaxConLife">
       <entry>Max Connection Lifetime</entry>
       <entry>Maximum lifetime for a connection. Use a time constant in an expression to define the
        time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is 30
         seconds, defined as follows: <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-AutoCommit">
       <entry>Auto Commit</entry>
       <entry>Determines whether auto-commit mode is enabled or disabled for the
         connection.<p>Disabled by default.</p></entry>
      </row>
      <row id="row-EnReadOnly">
       <entry>Enforce Read-only Connection</entry>
       <entry>Creates read-only connections to avoid any type of write. <p>Enabled by default.
         Disabling this property is not recommended. </p></entry>
      </row>
      <row id="row-TransactionIsolation">
       <entry>Transaction Isolation</entry>
       <entry>Transaction isolation level used to connect to the database. <p>Default is the
        default transaction isolation level set for the database. You can override the database
        default by setting the level to any of the following:</p><ul id="ul_arc_mdf_2y">
         <li>Read committed </li>
         <li>Read uncommitted </li>
         <li>Repeatable read</li>
         <li>Serializable</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor"><b>table-S3AdvProps</b> Amazon S3 destination uses the whole
   table, S3 and Kinesis Consumer use certain rows.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table-S3AdvProps">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Advanced Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-UseProxy">
      <entry>Use Proxy</entry>
      <entry>Specifies whether to use a proxy to connect to Amazon S3.</entry>
     </row>
     <row id="row-ProxyHost">
      <entry>Proxy Host</entry>
      <entry>Proxy host.</entry>
     </row>
     <row id="row-ProxyPort">
      <entry>Proxy Port</entry>
      <entry>Proxy port.</entry>
     </row>
     <row id="row-ProxyUser">
      <entry>Proxy User</entry>
      <entry>User name for proxy credentials.</entry>
     </row>
     <row id="row-ProxyPassword">
      <entry>Proxy Password</entry>
      <entry>Password for proxy credentials.<note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Tip_Usernames"/></entry>
     </row>
     <row>
      <entry>Thread Pool Size for Parallel Uploads</entry>
      <entry>Size of the thread pool for parallel uploads. Used when writing to multiple partitions
       and writing large objects in multiple parts.<p>When writing to multiple partitions, setting
        this property up to the number of partitions being written to can improve performance.
        </p><p>For more information about this and the following properties, see the Amazon S3
        TransferManager documentation.</p></entry>
     </row>
     <row>
      <entry>Multipart Upload Threshold</entry>
      <entry>Minimum batch size in bytes for the destination to use multipart uploads.</entry>
     </row>
     <row>
      <entry>Minimum Upload Part Size</entry>
      <entry>Minimum part size in bytes for multipart uploads.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase, Redis, and Static Lookup processors use rows in the
   table. See below for same properties worded slightly differently for JDBC Lookup. Make same
   changes in both places.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_ns1_p1s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Lookup Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-Mode">
      <entry>Mode</entry>
      <entry>Mode used to perform the lookups:<ul id="ul_t11_3fs_zv">
       <li>Per Batch - Performs a bulk lookup of all keys in a
        batch. The processor performs a single lookup for
        each batch.</li>
       <li>Per Key in Each Record - Performs individual lookups
        of each key in each record. If you configure
        multiple key expressions, the processor performs
        multiple lookups for each record.</li>
      </ul><p>Default is Per Batch.</p></entry>
     </row>
     <row>
      <entry>Enable Local Caching </entry>
      <entry id="entry-LocalCaching">Specifies whether to locally cache the returned key-value
       pairs.</entry>
     </row>
     <row id="row-MaxEntriesCache">
      <entry>Maximum Entries to Cache</entry>
      <entry>Maximum number of key-value pairs to cache. When the
       maximum number is reached, the processor evicts the oldest
       key-value pairs from the cache. <p>Default is -1, which
        means unlimited.</p></entry>
     </row>
     <row id="row-EvictionPolicy">
      <entry>Eviction Policy Type</entry>
      <entry>Policy used to evict key-value pairs from the local cache
       when the expiration time has passed:<ul id="ul_jql_yns_zv">
        <li>Expire After Last Access - Measures the expiration
         time since the key-value pair was last accessed by a
         read or a write.</li>
        <li>Expire After Last Write - Measures the expiration
         time since the key-value pair was created, or since
         the value was last replaced.</li>
       </ul></entry>
     </row>
     <row id="row-ExpirationTime">
      <entry>Expiration Time</entry>
      <entry>Amount of time that a key-value pair can remain in the
       local cache without being accessed or written to. <p>Default
        is 1 second.</p></entry>
     </row>
     <row id="row-timeUnit">
      <entry>Time Unit</entry>
      <entry>Unit of time for the expiration time. <p>Default is
       seconds.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">JDBC Lookup and Salesforce Lookup use rows in this
   table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_ns3_p7s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Lookup Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Enable Local Caching </entry>
      <entry id="entry-JDBCLocalCaching">Specifies whether to locally cache the returned
       values.</entry>
     </row>
     <row id="row-JDBCMaxEntriesCache">
      <entry>Maximum Entries to Cache</entry>
      <entry>Maximum number of values to cache. When the maximum number is reached, the processor
       evicts the oldest values from the cache. <p>Default is -1, which means unlimited.</p></entry>
     </row>
     <row id="row-JDBCEvictionPolicy">
      <entry>Eviction Policy Type</entry>
      <entry>Policy used to evict values from the local cache when the expiration time has
        passed:<ul id="ul_jik_yns_zv">
        <li>Expire After Last Access - Measures the expiration time since the value was last
         accessed by a read or a write.</li>
        <li>Expire After Last Write - Measures the expiration time since the value was created, or
         since the value was last replaced.</li>
       </ul></entry>
     </row>
     <row id="row-JDBCExpirationTime">
      <entry>Expiration Time</entry>
      <entry>Amount of time that a value can remain in the local cache without being accessed or
       written to. <p>Default is 1 second.</p></entry>
     </row>
     <row id="row-JDBCtimeUnit">
      <entry>Time Unit</entry>
      <entry>Unit of time for the expiration time. <p>Default is
       seconds.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase destination and HBase Lookup use rows and entries in
   this table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_bgt_kly_bw">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>HBase Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-ZooKeeperQuorum">
      <entry>ZooKeeper Quorum</entry>
      <entry>Comma-separated list of servers in the ZooKeeper quorum. Use the following format:
        <codeblock>&lt;host>.&lt;domain>.com</codeblock><p>To ensure a connection, enter additional
        broker URIs.</p></entry>
     </row>
     <row id="row-ZooKeeperClient">
      <entry>ZooKeeper Client Port</entry>
      <entry>Port number clients use to connect to the ZooKeeper servers. </entry>
     </row>
     <row id="row-ZooKeeperParent">
      <entry>ZooKeeper Parent Znode</entry>
      <entry>Root node that contains all znodes used by the HBase cluster.</entry>
     </row>
     <row id="row-TableName">
      <entry>Table Name</entry>
      <entry>Name of the HBase table to use. Enter a table name or a namespace and table name as
       follows: &lt;namespace>.&lt;tablename>. <p>If you do not enter a table name, HBase uses the
        default namespace. </p></entry>
     </row>
     <row>
      <entry>Kerberos Authentication </entry>
      <entry id="entry-Kerberos">Uses Kerberos credentials to connect to HBase.<p>When selected,
        uses the Kerberos principal and keytab defined in the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
        configuration file, <codeph>$SDC_CONF/sdc.properties</codeph>. </p></entry>
     </row>
     <row>
      <entry>HBase User </entry>
      <entry id="entry-HBaseUser">The HBase user to use to connect to HBase. When using this
       property, make sure HBase is configured appropriately.<p>By default, the pipeline uses the
         <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> user
        to connect to HBase.</p></entry>
     </row>
     <row>
      <entry>HBase Configuration Directory</entry>
      <entry id="entry-HBaseConfigDirectory">Location of the HDFS configuration files. <p>For a
        Cloudera Manager installation, enter <codeph>hbase-conf</codeph>. For all other
        installations, use a directory or symlink within the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> resources
        directory.</p><p>You can use the following file with HBase:<ul
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HBasedest"
         id="ul_ezj_cvr_bt">
         <li/>
        </ul></p><note>Properties in the configuration files are overridden by individual properties
        defined in the stage.</note></entry>
     </row>
     <row>
      <entry>HBase Configuration</entry>
      <entry id="entry-HBaseConfig">
       <p>Additional HBase configuration properties to use. </p>
       <p>To add properties, click <uicontrol>Add</uicontrol> and
        define the property name and value. Use the property
        names and values as expected by HBase. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Redis origin, destination, and Lookup processor use this
   row</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_i5s_54s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Redis Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-RedisURI">
      <entry>URI</entry>
      <entry>URI of the Redis server. Use the following
        format:<codeblock>redis://&lt;host name>:&lt;port number>/&lt;database></codeblock><p>You
        can omit the database if the server uses the default database.</p><p>You can optionally
        include your password to log in to the Redis server. For
        example:<codeblock>redis://:&lt;password>@&lt;host name>:&lt;port number>/&lt;database></codeblock></p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">MongoDB origin and destination use all rows in this table.
   They use the description for the Enable SSL property</draft-comment>
    <table frame="all" rowsep="1" colsep="1" id="table_xwz_jsd_3t">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.5*"/>
      <colspec colname="c2" colnum="2" colwidth="3.5*"/>
      <thead>
       <row>
        <entry>Advanced Property</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row id="MongoDB_Connections_row">
      <entry>Connections Per Host</entry>
      <entry>Maximum number of connections for each host.<p>Default is 100.</p></entry>
     </row>
       <row id="MongoDB_MinConnections_row">
      <entry>Min Connections Per Host</entry>
      <entry>Minimum number of connections for each host.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_ConnectionTimeout_row">
      <entry>Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection. <p>Default is
       10,000.</p></entry>
     </row>
       <row id="MongoDB_MaxConnIdleTime_row">
      <entry>Max Connection Idle Time</entry>
      <entry>Maximum time in milliseconds that a pooled connection can remain idle. When a pooled
       connection exceeds the idle time, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxConnLifetime_row">
      <entry>Max Connection Lifetime</entry>
      <entry>Maximum time in milliseconds that a pooled connection can be active. When a pooled
       connection exceeds the lifetime, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxWaitTime_row">
      <entry>Max Wait Time</entry>
      <entry>Maximum time in milliseconds that a thread can wait for a connection to become
       available. Use 0 to opt out of this property. Use a negative value to wait
        indefinitely.<p>Default is 120,000.</p></entry>
     </row>
       <row id="MongoDB_ServerTimeout_row">
      <entry>Server Selection Timeout</entry>
      <entry>Maximum time in milliseconds that <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits for a server selection
       before throwing an exception. If you use 0, an exception is thrown immediately if no server
       is available. Use a negative value to wait indefinitely.<p>Default is 30,000.</p></entry>
     </row>
       <row id="MongoDB_ThreadsAllowed_row">
      <entry>Threads Allowed to Block for Connection Multiplier</entry>
      <entry>Multiplier that determines the maximum number of threads that can wait for a connection
       to become available from the pool. This number multiplied by the Connections Per Host value
       determines the maximum number of threads.<p>Default is 5.</p></entry>
     </row>
       <row id="MongoDB_Heartbeat_row">
      <entry>Heartbeat Frequency</entry>
      <entry>The frequency in milliseconds at which <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> attempts to determine the
       current state of each server in the cluster.<p>Default is 10,000.</p></entry>
     </row>
       <row id="MongoDB_MinHeartbeat_row">
      <entry>Min Heartbeat Frequency</entry>
      <entry>Minimum heartbeat frequency in milliseconds. <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits at least this long
       before checking the state of each server.<p>Default is 500.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatConn_row">
      <entry>Heartbeat Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatSocket_row">
      <entry>Heartbeat Socket Timeout</entry>
      <entry>Maximum time in milliseconds for a socket timeout for connections used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_LocalThreshold_row">
      <entry>Local Threshold</entry>
      <entry>Local threshold in milliseconds. Requests are sent to a server whose ping time is less
       than or equal to the server with the fastest ping time plus the local threshold
        value.<p>Default is 15.</p></entry>
     </row>
       <row id="MongoDB_Replica_row">
      <entry>Required Replica Set Name</entry>
      <entry>Required replica set name to use for the cluster.</entry>
     </row>
       <row id="MongoDB_Cursor_row">
      <entry>Cursor Finalizer Enabled</entry>
      <entry>Specifies whether to enable cursor finalizers.</entry>
     </row>
       <row id="MongoDB_SocketKeepAlive_row">
      <entry>Socket Keep Alive</entry>
      <entry>Specifies whether to enable socket keep alive. </entry>
     </row>
       <row id="MongoDB_SocketTimeout_row">
      <entry>Socket Timeout</entry>
      <entry>Maximum time in milliseconds for the socket timeout. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_SSLInvalid_row">
      <entry>SSL Invalid Host Name Allowed</entry>
      <entry>Specifies whether invalid host names are allowed in SSL/TLS certificates.</entry>
     </row>
      </tbody>
     </tgroup>
    </table>
  <draft-comment author="alisontaylor">Salesforce origin, Salesforce Lookup processor, and
   Salesforce destination use the first three rows in this table, and use the description of the
   last row. Wave Analytics destination uses the first two rows in the table.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_fg1_cbc_tx">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Salesforce Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="SalesforceUser">
      <entry>Username</entry>
      <entry>Salesforce username in the following email format:
       <codeph>&lt;text>@&lt;text>.com</codeph>. </entry>
     </row>
     <row id="SalesforcePassword">
      <entry>Password</entry>
      <entry>Salesforce password. <note
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/Tip_Usernames"/></entry>
     </row>
     <row id="SalesforceEndpoint">
      <entry>Auth Endpoint</entry>
      <entry>Salesforce SOAP API authentication endpoint. Enter one of the following values:<ul
        id="ul_lnr_x2c_tx">
        <li>login.salesforce.com - To connect to a Production or Developer Edition
         organization.</li>
        <li>test.salesforce.com - To connect to a sandbox organization.</li>
       </ul><p>Default is login.salesforce.com.</p></entry>
     </row>
     <row>
      <entry>API Version</entry>
      <entry id="SalesforceAPIVersion">Salesforce API version to use to connect to Salesforce.
        <p>Default is 36.0. If you change the version, you also must download the relevant JAR files
        from Salesforce Web Services Connector (WSC).</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Description of each role is conref'd in the LDAP and File
   Authentication sections. The whole table is conrefed by the Roles topic. (LC
   2/17)</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="Role_table">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Role</entry>
      <entry>Tasks</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>admin</entry>
      <entry id="entry_adminRole">Perform any <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> task. Can
       perform all tasks listed below, as well as start and stop <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>, view <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
       configuration, JVM metrics, and log information. Install libraries using the Package
       Manager.</entry>
     </row>
     <row>
      <entry>manager</entry>
      <entry id="entry_managerRole">Start and stop pipelines, monitor pipelines, configure and reset
       alerts. Take, review, and manage snapshots. </entry>
     </row>
     <row>
      <entry>creator</entry>
      <entry id="entry_creatorRole">Create and configure pipelines and alerts, preview data, and
       monitor the pipeline. Import pipelines.</entry>
     </row>
     <row>
      <entry>guest</entry>
      <entry id="entry_guestRole">View pipelines and alerts, and general monitoring information.
       Export a pipeline. </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">Permissions table - used in Pipeline Maintenance > Sharing
    Pipelines and Configuring > Users, Groups, blah blah > Pipeline Permissions</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="Permissions_pipe">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Permission</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>Read</entry>
       <entry>View and monitor the pipeline, and see alerts. View existing snapshot data.</entry>
      </row>
      <row>
       <entry>Write</entry>
       <entry>Edit the pipeline and alerts.</entry>
      </row>
      <row>
       <entry>Execute</entry>
       <entry>Start and stop the pipeline. Preview data and take a snapshot.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following User-Roles table used in Users, Groups, Roles,
    Perms > Users and also in File-Based Authentication > Step 2.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="User-Roles">
    <tgroup cols="3">
     <colspec colname="newCol3" colnum="1" colwidth="1*"/>
     <colspec colname="c1" colnum="2" colwidth="1*"/>
     <colspec colname="c2" colnum="3" colwidth="3.0*"/>
     <thead>
      <row>
       <entry>User Login</entry>
       <entry>Role</entry>
       <entry>Tasks</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry><codeph>admin</codeph> / <codeph>admin</codeph></entry>
       <entry>Admin</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_adminRole"/>
      </row>
      <row>
       <entry><codeph>manager</codeph> / <codeph>manager</codeph></entry>
       <entry>Manager</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_managerRole"
       />
      </row>
      <row>
       <entry><codeph>creator</codeph> / <codeph>creator</codeph></entry>
       <entry>Creator</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_creatorRole"
       />
      </row>
      <row>
       <entry><codeph>guest</codeph> / <codeph>guest</codeph></entry>
       <entry>Guest</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_guestRole"/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">The following User-Roles table used in Users, Groups, Roles, Perms
   > Users and also in File-Based Authentication > Step 2.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="User-Groups">
   <tgroup cols="3">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="2.0*"/>
    <colspec colname="newCol3" colnum="3" colwidth="1.5*"/>
    <thead>
     <row>
      <entry>User Login</entry>
      <entry>Roles</entry>
      <entry>Group</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><codeph>user1</codeph> / <codeph>user1</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>dev</entry>
     </row>
     <row>
      <entry><codeph>user2</codeph> / <codeph>user2</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>dev</entry>
     </row>
     <row>
      <entry><codeph>user3</codeph> / <codeph>user3</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>test</entry>
     </row>
     <row>
      <entry><codeph>user4</codeph> / <codeph>user4</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>test</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Descriptions are used in MapR DB JSON origin and
   destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_rcs_gcd_ty">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>MapR DB JSON Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Table Name</entry>
      <entry>
       <p id="MapRDBJSON_Table1">If you do not include a path to the table, the stage assumes that
        the table exists in the user's home directory. For example, <codeph>/user/&lt;user
         name>/&lt;table name></codeph>.</p>
       <p id="MapRDBJSON_Table2">You can include a path relative to the user's home directory or an absolute path when you
        enter the table name. For tables in a default cluster, specify the absolute path as
         <codeph>/&lt;table path></codeph>. For tables in a specific cluster, specify the absolute
        path as <codeph>/mapr/&lt;cluster name>/&lt;table path></codeph>.</p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </conbody>
</concept>
